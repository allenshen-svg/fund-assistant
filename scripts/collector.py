#!/usr/bin/env python3
"""
èˆ†æƒ…æ•°æ®é‡‡é›†å™¨ â€” åç«¯é‡‡é›†æ‰€æœ‰æ•°æ®æºï¼Œç¼“å­˜åˆ° JSON æ–‡ä»¶
æ”¯æŒ: æŠ–éŸ³/å¤´æ¡ / å¾®åš / ä¸œæ–¹è´¢å¯Œ / è´¢è”ç¤¾ / æ–°æµªè´¢ç» / çŸ¥ä¹ / ç™¾åº¦ / Bç«™
"""

import json, re, os, time, hashlib, traceback
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests

# ==================== å¸¸é‡ ====================
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')
CACHE_FILE = os.path.join(DATA_DIR, 'sentiment_cache.json')
UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
HEADERS = {'User-Agent': UA}
TIMEOUT = 15

# ==================== è´¢ç»å…³é”®è¯ ====================
FINANCE_KW = [
    'Aè‚¡','è‚¡å¸‚','å¤§ç›˜','æ²ªæŒ‡','ä¸Šè¯','æ·±æˆ','åˆ›ä¸šæ¿','ç§‘åˆ›æ¿','æ²ªæ·±300','æ’ç”Ÿ','æ¸¯è‚¡','ç¾è‚¡','çº³æ–¯è¾¾å…‹',
    'AI','äººå·¥æ™ºèƒ½','ç®—åŠ›','èŠ¯ç‰‡','åŠå¯¼ä½“','å…‰æ¨¡å—','CPO','å¤§æ¨¡å‹','DeepSeek',
    'æœºå™¨äºº','è‡ªåŠ¨é©¾é©¶','æ–°èƒ½æº','å…‰ä¼','é”‚ç”µ','ç¢³é…¸é”‚','å‚¨èƒ½',
    'å†›å·¥','å›½é˜²','èˆªå¤©','ç™½é…’','æ¶ˆè´¹','åŒ»è¯','åˆ›æ–°è¯','CXO',
    'é»„é‡‘','é‡‘ä»·','åŸæ²¹','æ²¹ä»·','æœ‰è‰²é‡‘å±','é“œ','é“','ç¨€åœŸ',
    'çº¢åˆ©','é«˜è‚¡æ¯','é“¶è¡Œ','ä¿é™©','åˆ¸å•†','åœ°äº§','æˆ¿ä»·','æ¥¼å¸‚','æˆ¿åœ°äº§',
    'å¤®è¡Œ','é™æ¯','é™å‡†','LPR','åˆ©ç‡','é€šèƒ€','CPI','GDP','PMI',
    'ç¾è”å‚¨','åŠ æ¯','å›½å€º','å€ºåˆ¸','æ±‡ç‡','äººæ°‘å¸',
    'å…³ç¨','è´¸æ˜“æˆ˜','åˆ¶è£',
    'åŸºé‡‘','ETF','ç‰›å¸‚','ç†Šå¸‚','æ¶¨åœ','è·Œåœ','æŠ„åº•','è¿½é«˜',
    'ä»“ä½','åŠ ä»“','å‡ä»“','å®šæŠ•','ä¸»åŠ›','èµ„é‡‘æµ','åŒ—å‘èµ„é‡‘',
    'èŒ…å°','æ¯”äºšè¿ª','å®å¾·','è‹±ä¼Ÿè¾¾','NVIDIA','ç‰¹æ–¯æ‹‰','æ ¼åŠ›','ä¸‡è¾¾',
    'IPO','åˆ†çº¢','å›è´­','å¹¶è´­','é‡ç»„','å‡æŒ','å¢æŒ',
    'æ¿å—','æŒ‡æ•°','æ¦‚å¿µè‚¡','é¢˜æ','é¾™å¤´è‚¡','ä¸»çº¿','èµ›é“',
    'å‡€åˆ©æ¶¦','è¥æ”¶','ä¸šç»©','å‡€å€¼','ä¼°å€¼','å¸‚ç›ˆç‡','å¸‚å€¼',
    'ç”µåŠ›','å†œä¸š','æ˜¥è€•',
    'ç§å‹Ÿ','å…¬å‹Ÿ','æœŸè´§','æœŸæƒ','åŸºæ°‘','è‚¡æ°‘','æ•£æˆ·',
    'ç›®æ ‡ä»·','è¯„çº§','ä¹°å…¥','å–å‡º','æŒæœ‰','çœ‹å¤š','çœ‹ç©º',
    'æŠ•èµ„è€…','èèµ„','èåˆ¸','æ æ†','åšç©º','åšå¤š','æ­¢æŸ',
    'è¯åˆ¸','ä¸Šå¸‚','æ¸¯äº¤æ‰€','å¤–æ±‡','åˆ©æ¶¦','äºæŸ','ç›ˆåˆ©','èµ„äº§','è´Ÿå€º',
    'ç»æµ','é‡‘è',
]
_kw_lower = [kw.lower() for kw in FINANCE_KW]

def is_finance(text):
    if not text:
        return False
    t = text.lower()
    return any(kw in t for kw in _kw_lower)

def estimate_sentiment(text):
    if not text:
        return 'ä¸­æ€§'
    if re.search(r'æš´æ¶¨|ç–¯æ¶¨|å¤§æ¶¨|é£™å‡|æ¶¨åœ|å…¨ä»“|æ¢­å“ˆ|èµ·é£|çˆ†å‘|ç‰›å¸‚|åˆ›æ–°é«˜|ç‹‚çƒ­', text):
        return 'æåº¦çœ‹å¤š'
    if re.search(r'ä¸Šæ¶¨|èµ°é«˜|åå¼¹|åˆ©å¥½|åŠ ä»“|æœºä¼š|çªç ´|çœ‹å¥½|æ¨è|é…ç½®|èµ°å¼º', text):
        return 'åå¤š'
    if re.search(r'æš´è·Œ|å´©ç›˜|å¤§è·Œ|è·³æ°´|æ¸…ä»“|å‰²è‚‰|ç†Šå¸‚|è…°æ–©', text):
        return 'æåº¦æ‚²è§‚'
    if re.search(r'ä¸‹è·Œ|èµ°ä½|åˆ©ç©º|å‡ä»“|é£é™©|è­¦æƒ•|è°¨æ…|å›è°ƒ|æ‰¿å‹|é‡æŒ«', text):
        return 'åç©º'
    if re.search(r'éœ‡è¡|åˆ†æ­§|è§‚æœ›|æŒå¹³|ç¨³å®š|ç›˜æ•´', text):
        return 'ä¸­æ€§'
    return 'ä¸­æ€§åå¤š'

def now_iso():
    return datetime.now(timezone.utc).isoformat()

def safe_int(v, default=0):
    try:
        return int(v)
    except (TypeError, ValueError):
        return default

# ==================== å„æ•°æ®æºé‡‡é›† ====================

def fetch_douyin():
    """æŠ–éŸ³çƒ­æœ + å¤´æ¡çƒ­æœ + å¤´æ¡è´¢ç»é¢‘é“ï¼ˆåŒå±å­—èŠ‚è·³åŠ¨ï¼‰â€” ç›®æ ‡ 30+"""
    items = []
    seen = set()

    # Source 1: æŠ–éŸ³çƒ­æœ API (finance filtered)
    try:
        r = requests.get('https://aweme.snssdk.com/aweme/v1/hot/search/list/',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        word_list = (data.get('data') or {}).get('word_list') or data.get('word_list') or []
        for item in word_list:
            word = item.get('word') or item.get('content') or ''
            hot = safe_int(item.get('hot_value') or item.get('score'))
            if word and is_finance(word) and word not in seen:
                seen.add(word)
                items.append({
                    'title': word[:80],
                    'summary': word,
                    'likes': hot,
                    'platform': 'æŠ–éŸ³',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(word),
                    'creator_type': 'ç¤¾äº¤çƒ­æœ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[æŠ–éŸ³] {e}')

    # Source 2: å¤´æ¡çƒ­æœ APIï¼ˆå­—èŠ‚è·³åŠ¨æ——ä¸‹ï¼‰(finance filtered)
    try:
        r = requests.get('https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('data') or []:
            title = item.get('Title') or ''
            hot = safe_int(item.get('HotValue'))
            if title and is_finance(title) and title not in seen:
                seen.add(title)
                items.append({
                    'title': title[:80],
                    'summary': title,
                    'likes': hot,
                    'platform': 'æŠ–éŸ³',
                    'source_type': 'å¤´æ¡çƒ­æœ',
                    'sentiment': estimate_sentiment(title),
                    'creator_type': 'ç¤¾äº¤çƒ­æœ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[å¤´æ¡] {e}')

    # Source 3: å¤´æ¡è´¢ç»é¢‘é“ä¿¡æ¯æµï¼ˆåˆ†é¡µè·å–ï¼Œæ¯é¡µ10æ¡ï¼Œå–3é¡µ=30æ¡ï¼‰
    max_behot = 0
    for page_idx in range(3):
        try:
            r = requests.get(
                f'https://www.toutiao.com/api/pc/feed/?category=news_finance&max_behot_time={max_behot}&widen=1&tadrequire=true',
                headers=HEADERS, timeout=TIMEOUT)
            data = r.json()
            feed_items = data.get('data') or []
            for item in feed_items:
                if not isinstance(item, dict):
                    continue
                title = (item.get('title') or '').strip()
                abstract = (item.get('abstract') or '').strip()
                if not title or title in seen:
                    continue
                seen.add(title)
                items.append({
                    'title': title[:80],
                    'summary': (abstract[:200] or title),
                    'likes': safe_int(item.get('hot', 0)),
                    'platform': 'æŠ–éŸ³',
                    'source_type': 'å¤´æ¡è´¢ç»',
                    'sentiment': estimate_sentiment(title + ' ' + abstract),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': now_iso(),
                })
                bt = item.get('behot_time', 0)
                if bt:
                    max_behot = bt
        except Exception as e:
            print(f'[å¤´æ¡è´¢ç»-page{page_idx+1}] {e}')
            break

    return items

def fetch_weibo():
    """å¾®åšçƒ­æœ â€” é€šè¿‡ Tophub èšåˆ API + å®˜æ–¹ APIï¼ˆè´¢ç»è¿‡æ»¤ï¼‰â€” ç›®æ ‡ 30+"""
    items = []
    seen = set()

    # Approach 1: Tophub èšåˆ APIï¼ˆç¨³å®šå¯ç”¨ï¼Œå…¨é‡é‡‡é›† ~51æ¡ï¼‰
    try:
        r = requests.get('https://api.codelife.cc/api/top/list?lang=cn&id=KqndgxeLl9',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('data') or []:
            word = (item.get('title') or '').strip()
            hot_str = item.get('hotValue') or ''
            # è§£æ "108ä¸‡" â†’ 1080000
            hot = 0
            m = re.match(r'([\d.]+)\s*ä¸‡', hot_str)
            if m:
                hot = int(float(m.group(1)) * 10000)
            else:
                hot = safe_int(re.sub(r'[^\d]', '', hot_str))
            if word and is_finance(word) and word not in seen:
                seen.add(word)
                items.append({
                    'title': word[:80],
                    'summary': word,
                    'likes': hot,
                    'platform': 'å¾®åš',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(word),
                    'creator_type': 'å¾®åšçƒ­æœ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[å¾®åš-tophub] {e}')

    # Approach 2: å®˜æ–¹ ajax API å…œåº•
    if len(items) < 30:
        try:
            r = requests.get('https://weibo.com/ajax/side/hotSearch',
                             headers=HEADERS, timeout=TIMEOUT)
            data = r.json()
            for item in (data.get('data') or {}).get('realtime') or []:
                word = item.get('word') or item.get('note') or ''
                hot = safe_int(item.get('raw_hot') or item.get('num'))
                if word and is_finance(word) and word not in seen:
                    seen.add(word)
                    items.append({
                        'title': word[:80],
                        'summary': word,
                        'likes': hot,
                        'platform': 'å¾®åš',
                        'source_type': 'çƒ­æœ',
                        'sentiment': estimate_sentiment(word),
                        'creator_type': 'å¾®åšçƒ­æœ',
                        'publish_time': now_iso(),
                    })
        except Exception as e:
            print(f'[å¾®åš-ajax] {e}')

    return items

def fetch_eastmoney():
    """ä¸œæ–¹è´¢å¯Œ 7x24 å¿«è®¯ + Tophub ä¸œæ–¹è´¢å¯Œçƒ­æ¦œ â€” ç›®æ ‡ 30+"""
    items = []
    seen = set()

    # Source 1: 7x24 å¿«è®¯ API (finance focused, ~10 items)
    try:
        url = f'https://np-listapi.eastmoney.com/comm/web/getNewsByColumns?client=web&biz=web_724&column=350&pageSize=50&maxNewsId=0&type=0&req_trace=sa_{int(time.time())}'
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('list') or []:
            title = (item.get('title') or '').strip()
            content = (item.get('content') or '').strip()
            text = title or content[:100]
            if text and len(text) >= 4 and text not in seen:
                seen.add(text)
                items.append({
                    'title': text[:80],
                    'summary': (content[:200] or text),
                    'likes': 0,
                    'platform': 'ä¸œæ–¹è´¢å¯Œ',
                    'source_type': 'å¿«è®¯',
                    'sentiment': estimate_sentiment(text + ' ' + content),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': item.get('showTime') or now_iso(),
                })
    except Exception as e:
        print(f'[ä¸œæ–¹è´¢å¯Œ-7x24] {e}')

    # Source 2: Tophub ä¸œæ–¹è´¢å¯Œçƒ­æ¦œ (~20 items)
    try:
        r = requests.get('https://api.codelife.cc/api/top/list?lang=cn&id=Y2KeDGQdNP',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('data') or []:
            title = (item.get('title') or '').strip()
            if title and title not in seen:
                seen.add(title)
                items.append({
                    'title': title[:80],
                    'summary': title,
                    'likes': 0,
                    'platform': 'ä¸œæ–¹è´¢å¯Œ',
                    'source_type': 'çƒ­æ¦œ',
                    'sentiment': estimate_sentiment(title),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[ä¸œæ–¹è´¢å¯Œ-tophub] {e}')

    return items

def fetch_cailian():
    """è´¢è”ç¤¾ç”µæŠ¥ â€” ç›®æ ‡ 50 æ¡"""
    items = []
    try:
        r = requests.get(
            'https://www.cls.cn/nodeapi/updateTelegraphList?app=CailianpressWeb&os=web&sv=8.4.6&rn=50',
            headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('roll_data') or []:
            title = (item.get('title') or '').strip()
            content = re.sub(r'<[^>]+>', '', item.get('content') or '').strip()
            text = title or content[:100]
            if text and len(text) >= 4:
                pub_time = now_iso()
                if item.get('ctime'):
                    pub_time = datetime.fromtimestamp(item['ctime'], tz=timezone.utc).isoformat()
                items.append({
                    'title': text[:80],
                    'summary': (content[:200] or text),
                    'likes': 0,
                    'platform': 'è´¢è”ç¤¾',
                    'source_type': 'ç”µæŠ¥',
                    'sentiment': estimate_sentiment(text + ' ' + content),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': pub_time,
                })
    except Exception as e:
        print(f'[è´¢è”ç¤¾] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_zhihu():
    """çŸ¥ä¹çƒ­æ¦œ â€” è´¢ç»è¿‡æ»¤ï¼Œç›®æ ‡ 50 æ¡"""
    items = []
    try:
        r = requests.get('https://api.zhihu.com/topstory/hot-lists/total?limit=50',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('data') or []:
            target = item.get('target') or {}
            title = target.get('title') or ''
            excerpt = target.get('excerpt') or ''
            detail = item.get('detail_text') or ''
            hot = safe_int(re.sub(r'[^\d]', '', detail))
            if title and is_finance(title + ' ' + excerpt):
                items.append({
                    'title': title[:80],
                    'summary': (excerpt[:200] or title),
                    'likes': hot,
                    'platform': 'çŸ¥ä¹',
                    'source_type': 'çƒ­æ¦œ',
                    'sentiment': estimate_sentiment(title + ' ' + excerpt),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[çŸ¥ä¹] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_baidu():
    """ç™¾åº¦çƒ­æœ (realtime è´¢ç»è¿‡æ»¤ + è´¢ç»é¢‘é“) â€” ç›®æ ‡ 30+"""
    items = []
    seen = set()

    def _parse_baidu(data):
        flat = []
        for card in (data.get('data') or {}).get('cards') or []:
            for c in card.get('content') or []:
                if isinstance(c.get('content'), list):
                    flat.extend(c['content'])
                elif c.get('word'):
                    flat.append(c)
        return flat

    # Source 1: å®æ—¶çƒ­æœï¼ˆè´¢ç»è¿‡æ»¤ï¼‰
    try:
        r = requests.get('https://top.baidu.com/api/board?platform=wise&tab=realtime',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in _parse_baidu(data):
            word = item.get('word') or ''
            desc = item.get('desc') or ''
            hot = safe_int(item.get('hotScore'))
            if word and is_finance(word + ' ' + desc) and word not in seen:
                seen.add(word)
                items.append({
                    'title': word[:80],
                    'summary': (desc[:200] or word),
                    'likes': hot,
                    'platform': 'ç™¾åº¦',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(word + ' ' + desc),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[ç™¾åº¦-realtime] {e}')

    # Source 2: è´¢ç»çƒ­æœï¼ˆè¡¥å……è´¢ç»ä¸“é¢˜ï¼‰
    if len(items) < 30:
        try:
            r = requests.get('https://top.baidu.com/api/board?platform=wise&tab=finance',
                             headers=HEADERS, timeout=TIMEOUT)
            data = r.json()
            for item in _parse_baidu(data):
                word = item.get('word') or ''
                desc = item.get('desc') or ''
                hot = safe_int(item.get('hotScore'))
                if word and word not in seen:
                    seen.add(word)
                    items.append({
                        'title': word[:80],
                        'summary': (desc[:200] or word),
                        'likes': hot,
                        'platform': 'ç™¾åº¦',
                        'source_type': 'è´¢ç»çƒ­æœ',
                        'sentiment': estimate_sentiment(word + ' ' + desc),
                        'creator_type': 'èšåˆçƒ­æ¦œ',
                        'publish_time': now_iso(),
                    })
        except Exception as e:
            print(f'[ç™¾åº¦-finance] {e}')

    return items

def fetch_bilibili():
    """Bç«™è´¢ç»é¢‘é“ + çƒ­æœ + æ’è¡Œ â€” ç›®æ ‡ 30+"""
    items = []
    seen = set()

    # Source 1: è´¢ç»é¢‘é“åŠ¨æ€ï¼ˆrid=207, äºŒæ¬¡è¿‡æ»¤ç¡®ä¿è´¢ç»ç›¸å…³ï¼‰~50æ¡
    try:
        r = requests.get('https://api.bilibili.com/x/web-interface/dynamic/region?rid=207&ps=50',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('archives') or []:
            title = (item.get('title') or '').strip()
            desc = (item.get('desc') or '').strip()
            views = (item.get('stat') or {}).get('view') or 0
            if title and is_finance(title + ' ' + desc) and title not in seen:
                seen.add(title)
                items.append({
                    'title': title[:80],
                    'summary': (desc[:200] or title),
                    'likes': safe_int(views),
                    'platform': 'Bç«™',
                    'source_type': 'è´¢ç»é¢‘é“',
                    'sentiment': estimate_sentiment(title + ' ' + desc),
                    'creator_type': 'è§†é¢‘ç¤¾åŒº',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[Bç«™-è´¢ç»é¢‘é“] {e}')

    # Source 2: çƒ­æœè¯ï¼ˆè´¢ç»è¿‡æ»¤ï¼‰
    try:
        r = requests.get('https://s.search.bilibili.com/main/hotword',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('list') or []:
            kw = item.get('keyword') or ''
            if kw and is_finance(kw) and kw not in seen:
                seen.add(kw)
                items.append({
                    'title': kw[:80],
                    'summary': kw,
                    'likes': safe_int(item.get('heat_score')),
                    'platform': 'Bç«™',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(kw),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[Bç«™-çƒ­æœ] {e}')

    # Source 3: å…¨ç«™æ’è¡Œæ¦œï¼ˆè´¢ç»è¿‡æ»¤ï¼‰
    try:
        r = requests.get('https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('list') or []:
            title = item.get('title') or ''
            desc = item.get('desc') or ''
            views = (item.get('stat') or {}).get('view') or 0
            if title and is_finance(title + ' ' + desc) and title not in seen:
                seen.add(title)
                items.append({
                    'title': title[:80],
                    'summary': (desc[:200] or title),
                    'likes': safe_int(views),
                    'platform': 'Bç«™',
                    'source_type': 'æ’è¡Œ',
                    'sentiment': estimate_sentiment(title + ' ' + desc),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[Bç«™-æ’è¡Œ] {e}')

    return items

def fetch_sina_finance():
    """æ–°æµªè´¢ç»çƒ­ç‚¹æ–°é—» â€” ç›®æ ‡ 50 æ¡"""
    items = []
    try:
        r = requests.get(
            'https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&k=&num=50&page=1',
            headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('result') or {}).get('data') or []:
            title = (item.get('title') or '').strip()
            summary = (item.get('intro') or item.get('summary') or '').strip()
            pub_time = now_iso()
            if item.get('ctime'):
                try:
                    pub_time = datetime.fromtimestamp(int(item['ctime']), tz=timezone.utc).isoformat()
                except: pass
            if title and len(title) >= 4:
                items.append({
                    'title': title[:80],
                    'summary': (summary[:200] or title),
                    'likes': 0,
                    'platform': 'æ–°æµªè´¢ç»',
                    'source_type': 'è´¢ç»æ–°é—»',
                    'sentiment': estimate_sentiment(title + ' ' + summary),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': pub_time,
                })
    except Exception as e:
        print(f'[æ–°æµªè´¢ç»] é‡‡é›†å¤±è´¥: {e}')
    return items

def _parse_xhs_ssr(html):
    """ä»å°çº¢ä¹¦ HTML ä¸­æå– __INITIAL_STATE__ SSR æ•°æ®."""
    m = re.search(r'window\.__INITIAL_STATE__\s*=\s*(.+?)</script>', html, re.DOTALL)
    if not m:
        return []
    raw = m.group(1).strip().rstrip(';').replace('undefined', 'null')
    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        return []
    feeds = data.get('feed', {}).get('feeds', [])
    if not feeds:
        feeds = data.get('homeFeed', {}).get('feeds', [])
    results = []
    for f in feeds:
        card = f.get('noteCard') or {}
        title = (card.get('displayTitle') or '').strip()
        if not title:
            continue
        interact = card.get('interactInfo', {})
        liked_str = interact.get('likedCount', '0')
        if isinstance(liked_str, str) and 'ä¸‡' in liked_str:
            liked = int(float(liked_str.replace('ä¸‡', '')) * 10000)
        else:
            liked = safe_int(liked_str)
        results.append({'title': title, 'likes': liked})
    return results


def fetch_xiaohongshu():
    """å°çº¢ä¹¦çƒ­é—¨ â€” explore SSR + é¢‘é“é¡µï¼Œé—´æ­‡æ€§å¯ç”¨ â€” ç›®æ ‡ 30+"""
    items = []
    seen = set()

    def _add(notes, source_type):
        for n in notes:
            t = n['title']
            if t in seen:
                continue
            seen.add(t)
            items.append({
                'title': t[:80],
                'summary': t,
                'likes': n['likes'],
                'platform': 'å°çº¢ä¹¦',
                'source_type': source_type,
                'sentiment': estimate_sentiment(t),
                'creator_type': 'å°çº¢ä¹¦åšä¸»',
                'publish_time': now_iso(),
            })

    # Source 1: explore é¦–é¡µ SSR
    try:
        r = requests.get('https://www.xiaohongshu.com/explore', headers=HEADERS, timeout=TIMEOUT)
        if r.status_code == 200 and len(r.text) > 50000:
            notes = _parse_xhs_ssr(r.text)
            _add(notes, 'çƒ­é—¨ç¬”è®°')
        else:
            print(f'[å°çº¢ä¹¦] explore è¿”å› {len(r.text)} å­—èŠ‚ (æ—  SSR)')
    except Exception as e:
        print(f'[å°çº¢ä¹¦-explore] {e}')

    # Source 2: é¢‘é“é¡µï¼ˆæ¨è/ç¾é£Ÿ/æ—…è¡Œç­‰ï¼Œå„é¢‘é“å†…å®¹ä¸åŒï¼‰
    if len(items) < 30:
        channels = ['homefeed_recommend', 'homefeed.food_v3', 'homefeed.travel_v3']
        for cid in channels:
            if len(items) >= 40:
                break
            try:
                r = requests.get(f'https://www.xiaohongshu.com/explore?channel_id={cid}',
                                 headers=HEADERS, timeout=TIMEOUT)
                if r.status_code == 200 and len(r.text) > 50000:
                    notes = _parse_xhs_ssr(r.text)
                    _add(notes, 'é¢‘é“çƒ­é—¨')
            except Exception as e:
                print(f'[å°çº¢ä¹¦-{cid}] {e}')

    if not items:
        print('[å°çº¢ä¹¦] SSR ä¸å¯ç”¨ï¼ˆå¯èƒ½è¢«é™æµï¼‰ï¼Œæœ¬æ¬¡è¿”å› 0 æ¡')

    return items


# ==================== éš”å¤œç¾è‚¡è¡Œæƒ… ====================
US_MARKET_CACHE = os.path.join(DATA_DIR, 'us_market_cache.json')

# è¿½è¸ªçš„ç¾è‚¡æ ‡çš„: (symbol, ä¸­æ–‡å, ç±»å‹)
US_SYMBOLS = [
    ('NVDA',  'è‹±ä¼Ÿè¾¾',    'åŠå¯¼ä½“'),
    ('AMD',   'AMD',      'åŠå¯¼ä½“'),
    ('AVGO',  'åšé€š',      'åŠå¯¼ä½“'),
    ('TSLA',  'ç‰¹æ–¯æ‹‰',    'æ–°èƒ½æºè½¦'),
    ('AAPL',  'è‹¹æœ',      'ç§‘æŠ€'),
    ('SOXX',  'åŠå¯¼ä½“ETF', 'åŠå¯¼ä½“æŒ‡æ•°'),
    ('.IXIC', 'çº³æ–¯è¾¾å…‹',  'ç¾è‚¡æŒ‡æ•°'),
    ('.INX',  'æ ‡æ™®500',   'ç¾è‚¡æŒ‡æ•°'),
    ('.DJI',  'é“ç¼æ–¯',    'ç¾è‚¡æŒ‡æ•°'),
]

def fetch_us_market():
    """ä»é›ªçƒè·å–éš”å¤œç¾è‚¡è¡Œæƒ… â€” åŠå¯¼ä½“ + ç§‘æŠ€ + ä¸‰å¤§æŒ‡æ•°"""
    try:
        s = requests.Session()
        s.headers.update({'User-Agent': UA})
        s.get('https://xueqiu.com/', timeout=5)  # è·å– cookie

        symbols = ','.join(sym for sym, _, _ in US_SYMBOLS)
        r = s.get(
            f'https://stock.xueqiu.com/v5/stock/realtime/quotec.json?symbol={symbols}',
            timeout=TIMEOUT
        )
        data = r.json()
        sym_map = {sym: (cn, cat) for sym, cn, cat in US_SYMBOLS}

        results = []
        for item in data.get('data', []):
            sym = item.get('symbol', '')
            cn, cat = sym_map.get(sym, (sym, 'å…¶ä»–'))
            results.append({
                'symbol': sym,
                'name': cn,
                'category': cat,
                'price': item.get('current'),
                'change': round(item.get('chg', 0), 2),
                'percent': round(item.get('percent', 0), 2),
                'amplitude': item.get('amplitude'),
                'high': item.get('high'),
                'low': item.get('low'),
                'volume': item.get('volume'),
                'market_cap': item.get('market_capital'),
                'timestamp': item.get('timestamp'),
            })

        # ä¿å­˜ç¼“å­˜
        cache = {
            'stocks': results,
            'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'fetch_ts': int(time.time()),
        }
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(US_MARKET_CACHE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        print(f'  ğŸ‡ºğŸ‡¸ ç¾è‚¡è¡Œæƒ…: {len(results)} ä¸ªæ ‡çš„å·²ç¼“å­˜')
        return cache
    except Exception as e:
        print(f'  âš ï¸ ç¾è‚¡è¡Œæƒ…é‡‡é›†å¤±è´¥: {e}')
        return None

def load_us_market_cache():
    """è¯»å–ç¾è‚¡è¡Œæƒ…ç¼“å­˜"""
    if not os.path.exists(US_MARKET_CACHE):
        return None
    try:
        with open(US_MARKET_CACHE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None


# ==================== å»é‡ ====================
def dedup(items):
    seen = set()
    result = []
    for item in items:
        key = re.sub(r'[\W\s]', '', (item.get('title') or ''))[:20]
        if not key or key in seen:
            continue
        seen.add(key)
        result.append(item)
    return result

# ==================== ä¸»é‡‡é›†æµç¨‹ ====================
ALL_FETCHERS = [
    ('æŠ–éŸ³', fetch_douyin),
    ('å¾®åš', fetch_weibo),
    ('ä¸œæ–¹è´¢å¯Œ', fetch_eastmoney),
    ('è´¢è”ç¤¾', fetch_cailian),
    ('æ–°æµªè´¢ç»', fetch_sina_finance),
    ('çŸ¥ä¹', fetch_zhihu),
    ('ç™¾åº¦', fetch_baidu),
    ('Bç«™', fetch_bilibili),
    ('å°çº¢ä¹¦', fetch_xiaohongshu),
]

def collect_all():
    """å¹¶è¡Œé‡‡é›†æ‰€æœ‰æ•°æ®æºï¼Œè¿”å› { items, source_counts, fetch_time, ... }"""
    all_items = []
    source_counts = {}
    errors = []

    print(f'[{datetime.now().strftime("%H:%M:%S")}] å¼€å§‹é‡‡é›† {len(ALL_FETCHERS)} ä¸ªæ•°æ®æº...')

    with ThreadPoolExecutor(max_workers=len(ALL_FETCHERS)) as executor:
        futures = {executor.submit(fn): name for name, fn in ALL_FETCHERS}
        for future in as_completed(futures):
            name = futures[future]
            try:
                items = future.result(timeout=20)
                source_counts[name] = len(items)
                all_items.extend(items)
                print(f'  âœ… {name}: {len(items)} æ¡')
            except Exception as e:
                source_counts[name] = 0
                errors.append(f'{name}: {str(e)}')
                print(f'  âŒ {name}: {e}')

    # å»é‡ + æ’åº
    all_items = dedup(all_items)
    all_items.sort(key=lambda x: x.get('likes', 0), reverse=True)

    result = {
        'items': all_items,
        'source_counts': source_counts,
        'total': len(all_items),
        'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'fetch_ts': int(time.time()),
        'errors': errors,
    }

    print(f'  ğŸ“Š å…±è®¡ {len(all_items)} æ¡ (å»é‡å)')
    return result

def save_cache(data):
    """å°†é‡‡é›†ç»“æœä¿å­˜åˆ° JSON ç¼“å­˜æ–‡ä»¶"""
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f'  ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {CACHE_FILE}')

def load_cache():
    """è¯»å–ç¼“å­˜æ–‡ä»¶"""
    if not os.path.exists(CACHE_FILE):
        return None
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None

def collect_and_save(run_analysis=True):
    """é‡‡é›†å¹¶ä¿å­˜ï¼Œå¯é€‰è‡ªåŠ¨è¿è¡Œ AI åˆ†æ â€” ä¾› cron æˆ– server è°ƒç”¨"""
    # å…ˆé‡‡é›†ç¾è‚¡è¡Œæƒ…ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰
    us_data = fetch_us_market()
    data = collect_all()
    save_cache(data)
    if run_analysis and data.get('items'):
        try:
            import importlib, sys
            # Ensure scripts dir is on path for sibling import
            scripts_dir = os.path.dirname(os.path.abspath(__file__))
            if scripts_dir not in sys.path:
                sys.path.insert(0, scripts_dir)
            from analyzer import analyze_and_save
            analyze_and_save(data['items'])
        except Exception as e:
            print(f'  âš ï¸ AI åˆ†æé˜¶æ®µå‡ºé”™: {e}')
    return data

# ==================== CLI ====================
if __name__ == '__main__':
    data = collect_and_save()
    print(f'\né‡‡é›†å®Œæˆ: {data["total"]} æ¡, æ—¶é—´: {data["fetch_time"]}')
    for name, count in data['source_counts'].items():
        print(f'  {name}: {count}')
