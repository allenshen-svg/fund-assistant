#!/usr/bin/env python3
"""
èˆ†æƒ…æ•°æ®é‡‡é›†å™¨ â€” åç«¯é‡‡é›†æ‰€æœ‰æ•°æ®æºï¼Œç¼“å­˜åˆ° JSON æ–‡ä»¶
æ”¯æŒ: æŠ–éŸ³ / å¾®åš / ä¸œæ–¹è´¢å¯Œ / è´¢è”ç¤¾ / çŸ¥ä¹ / ç™¾åº¦ / Bç«™
"""

import json, re, os, time, hashlib, traceback
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests

# ==================== å¸¸é‡ ====================
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')
CACHE_FILE = os.path.join(DATA_DIR, 'sentiment_cache.json')
UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
HEADERS = {'User-Agent': UA}
TIMEOUT = 15

# ==================== è´¢ç»å…³é”®è¯ ====================
FINANCE_KW = [
    'Aè‚¡','è‚¡å¸‚','å¤§ç›˜','æ²ªæŒ‡','ä¸Šè¯','æ·±æˆ','åˆ›ä¸šæ¿','ç§‘åˆ›æ¿','æ²ªæ·±300','æ’ç”Ÿ','æ¸¯è‚¡','ç¾è‚¡','çº³æ–¯è¾¾å…‹',
    'AI','äººå·¥æ™ºèƒ½','ç®—åŠ›','èŠ¯ç‰‡','åŠå¯¼ä½“','å…‰æ¨¡å—','CPO','å¤§æ¨¡å‹','DeepSeek',
    'æœºå™¨äºº','è‡ªåŠ¨é©¾é©¶','æ–°èƒ½æº','å…‰ä¼','é”‚ç”µ','ç¢³é…¸é”‚','å‚¨èƒ½',
    'å†›å·¥','å›½é˜²','èˆªå¤©','ç™½é…’','æ¶ˆè´¹','åŒ»è¯','åˆ›æ–°è¯','CXO',
    'é»„é‡‘','é‡‘ä»·','åŸæ²¹','æ²¹ä»·','æœ‰è‰²é‡‘å±','é“œ','é“','ç¨€åœŸ',
    'çº¢åˆ©','é«˜è‚¡æ¯','é“¶è¡Œ','ä¿é™©','åˆ¸å•†','åœ°äº§',
    'å¤®è¡Œ','é™æ¯','é™å‡†','LPR','åˆ©ç‡','é€šèƒ€','CPI','GDP','PMI',
    'ç¾è”å‚¨','åŠ æ¯','å›½å€º','å€ºåˆ¸','æ±‡ç‡','äººæ°‘å¸',
    'å…³ç¨','è´¸æ˜“æˆ˜','åˆ¶è£','åœ°ç¼˜','ä¸­ä¸œ','ä¿„ä¹Œ',
    'åŸºé‡‘','ETF','ç‰›å¸‚','ç†Šå¸‚','æ¶¨åœ','è·Œåœ','æŠ„åº•','è¿½é«˜',
    'ä»“ä½','åŠ ä»“','å‡ä»“','å®šæŠ•','ä¸»åŠ›','èµ„é‡‘','åŒ—å‘',
    'èŒ…å°','æ¯”äºšè¿ª','å®å¾·','è‹±ä¼Ÿè¾¾','NVIDIA','ç‰¹æ–¯æ‹‰',
    'IPO','åˆ†çº¢','å›è´­','å¹¶è´­','é‡ç»„','è‚¡','åŸº','å¸‚åœº','ç»æµ','æŠ•èµ„','æ”¶ç›Š','è¡Œæƒ…',
    'æ¿å—','æŒ‡æ•°','æ¦‚å¿µ','é¢˜æ','é¾™å¤´','ä¸»çº¿','èµ›é“',
]
_kw_lower = [kw.lower() for kw in FINANCE_KW]

def is_finance(text):
    if not text:
        return False
    t = text.lower()
    return any(kw in t for kw in _kw_lower)

def estimate_sentiment(text):
    if not text:
        return 'ä¸­æ€§'
    if re.search(r'æš´æ¶¨|ç–¯æ¶¨|å¤§æ¶¨|é£™å‡|æ¶¨åœ|å…¨ä»“|æ¢­å“ˆ|èµ·é£|çˆ†å‘|ç‰›å¸‚|åˆ›æ–°é«˜|ç‹‚çƒ­', text):
        return 'æåº¦çœ‹å¤š'
    if re.search(r'ä¸Šæ¶¨|èµ°é«˜|åå¼¹|åˆ©å¥½|åŠ ä»“|æœºä¼š|çªç ´|çœ‹å¥½|æ¨è|é…ç½®|èµ°å¼º', text):
        return 'åå¤š'
    if re.search(r'æš´è·Œ|å´©ç›˜|å¤§è·Œ|è·³æ°´|æ¸…ä»“|å‰²è‚‰|ç†Šå¸‚|è…°æ–©', text):
        return 'æåº¦æ‚²è§‚'
    if re.search(r'ä¸‹è·Œ|èµ°ä½|åˆ©ç©º|å‡ä»“|é£é™©|è­¦æƒ•|è°¨æ…|å›è°ƒ|æ‰¿å‹|é‡æŒ«', text):
        return 'åç©º'
    if re.search(r'éœ‡è¡|åˆ†æ­§|è§‚æœ›|æŒå¹³|ç¨³å®š|ç›˜æ•´', text):
        return 'ä¸­æ€§'
    return 'ä¸­æ€§åå¤š'

def now_iso():
    return datetime.now(timezone.utc).isoformat()

def safe_int(v, default=0):
    try:
        return int(v)
    except (TypeError, ValueError):
        return default

# ==================== å„æ•°æ®æºé‡‡é›† ====================

def fetch_douyin():
    """æŠ–éŸ³çƒ­æœ"""
    items = []
    try:
        r = requests.get('https://aweme.snssdk.com/aweme/v1/hot/search/list/',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        word_list = (data.get('data') or {}).get('word_list') or data.get('word_list') or []
        for item in word_list:
            word = item.get('word') or item.get('content') or ''
            hot = safe_int(item.get('hot_value') or item.get('score'))
            if word and is_finance(word):
                items.append({
                    'title': word[:80],
                    'summary': word,
                    'likes': hot,
                    'platform': 'æŠ–éŸ³',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(word),
                    'creator_type': 'ç¤¾äº¤çƒ­æœ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[æŠ–éŸ³] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_weibo():
    """å¾®åšçƒ­æœ â€” å°è¯•å¤šç§æ–¹å¼"""
    items = []
    # Approach 1: ajax API
    try:
        r = requests.get('https://weibo.com/ajax/side/hotSearch',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('realtime') or []:
            word = item.get('word') or item.get('note') or ''
            hot = safe_int(item.get('raw_hot') or item.get('num'))
            if word and is_finance(word):
                items.append({
                    'title': word[:80],
                    'summary': word,
                    'likes': hot,
                    'platform': 'å¾®åš',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(word),
                    'creator_type': 'å¾®åšçƒ­æœ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[å¾®åš-ajax] {e}')

    # Approach 2: mobile API
    if not items:
        try:
            r = requests.get(
                'https://m.weibo.cn/api/container/getIndex?containerid=106003type%3D25%26t%3D3%26disable_hot%3D1%26filter_type%3Drealtimehot',
                headers=HEADERS, timeout=TIMEOUT)
            data = r.json()
            for card in (data.get('data') or {}).get('cards') or []:
                for g in card.get('card_group') or []:
                    word = g.get('desc') or ''
                    hot = safe_int(g.get('desc_extr'))
                    if word and is_finance(word):
                        items.append({
                            'title': word[:80],
                            'summary': word,
                            'likes': hot,
                            'platform': 'å¾®åš',
                            'source_type': 'çƒ­æœ',
                            'sentiment': estimate_sentiment(word),
                            'creator_type': 'å¾®åšçƒ­æœ',
                            'publish_time': now_iso(),
                        })
        except Exception as e:
            print(f'[å¾®åš-mobile] {e}')

    return items

def fetch_eastmoney():
    """ä¸œæ–¹è´¢å¯Œ 7x24 å¿«è®¯"""
    items = []
    try:
        url = f'https://np-listapi.eastmoney.com/comm/web/getNewsByColumns?client=web&biz=web_724&column=350&pageSize=50&maxNewsId=0&type=0&req_trace=sa_{int(time.time())}'
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('list') or []:
            title = (item.get('title') or '').strip()
            content = (item.get('content') or '').strip()
            text = title or content[:100]
            if text and len(text) >= 4:
                items.append({
                    'title': text[:80],
                    'summary': (content[:200] or text),
                    'likes': 0,
                    'platform': 'ä¸œæ–¹è´¢å¯Œ',
                    'source_type': 'å¿«è®¯',
                    'sentiment': estimate_sentiment(text + ' ' + content),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': item.get('showTime') or now_iso(),
                })
    except Exception as e:
        print(f'[ä¸œæ–¹è´¢å¯Œ] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_cailian():
    """è´¢è”ç¤¾ç”µæŠ¥"""
    items = []
    try:
        r = requests.get(
            'https://www.cls.cn/nodeapi/updateTelegraphList?app=CailianpressWeb&os=web&sv=8.4.6&rn=30',
            headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('roll_data') or []:
            title = (item.get('title') or '').strip()
            content = re.sub(r'<[^>]+>', '', item.get('content') or '').strip()
            text = title or content[:100]
            if text and len(text) >= 4:
                pub_time = now_iso()
                if item.get('ctime'):
                    pub_time = datetime.fromtimestamp(item['ctime'], tz=timezone.utc).isoformat()
                items.append({
                    'title': text[:80],
                    'summary': (content[:200] or text),
                    'likes': 0,
                    'platform': 'è´¢è”ç¤¾',
                    'source_type': 'ç”µæŠ¥',
                    'sentiment': estimate_sentiment(text + ' ' + content),
                    'creator_type': 'è´¢ç»èµ„è®¯å¹³å°',
                    'publish_time': pub_time,
                })
    except Exception as e:
        print(f'[è´¢è”ç¤¾] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_zhihu():
    """çŸ¥ä¹çƒ­æ¦œ"""
    items = []
    try:
        r = requests.get('https://api.zhihu.com/topstory/hot-lists/total?limit=50',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('data') or []:
            target = item.get('target') or {}
            title = target.get('title') or ''
            excerpt = target.get('excerpt') or ''
            detail = item.get('detail_text') or ''
            hot = safe_int(re.sub(r'[^\d]', '', detail))
            if title and is_finance(title + ' ' + excerpt):
                items.append({
                    'title': title[:80],
                    'summary': (excerpt[:200] or title),
                    'likes': hot,
                    'platform': 'çŸ¥ä¹',
                    'source_type': 'çƒ­æ¦œ',
                    'sentiment': estimate_sentiment(title + ' ' + excerpt),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[çŸ¥ä¹] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_baidu():
    """ç™¾åº¦çƒ­æœ"""
    items = []
    try:
        r = requests.get('https://top.baidu.com/api/board?platform=wise&tab=realtime',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        flat_list = []
        for card in (data.get('data') or {}).get('cards') or []:
            for c in card.get('content') or []:
                if isinstance(c.get('content'), list):
                    flat_list.extend(c['content'])
                elif c.get('word'):
                    flat_list.append(c)
        for item in flat_list:
            word = item.get('word') or ''
            desc = item.get('desc') or ''
            hot = safe_int(item.get('hotScore'))
            if word and is_finance(word + ' ' + desc):
                items.append({
                    'title': word[:80],
                    'summary': (desc[:200] or word),
                    'likes': hot,
                    'platform': 'ç™¾åº¦',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(word + ' ' + desc),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[ç™¾åº¦] é‡‡é›†å¤±è´¥: {e}')
    return items

def fetch_bilibili():
    """Bç«™çƒ­æœ + æ’è¡Œ"""
    items = []
    try:
        # çƒ­æœè¯
        r = requests.get('https://s.search.bilibili.com/main/hotword',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in data.get('list') or []:
            kw = item.get('keyword') or ''
            if kw and is_finance(kw):
                items.append({
                    'title': kw[:80],
                    'summary': kw,
                    'likes': safe_int(item.get('heat_score')),
                    'platform': 'Bç«™',
                    'source_type': 'çƒ­æœ',
                    'sentiment': estimate_sentiment(kw),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[Bç«™-çƒ­æœ] {e}')

    try:
        # æ’è¡Œæ¦œ
        r = requests.get('https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all',
                         headers=HEADERS, timeout=TIMEOUT)
        data = r.json()
        for item in (data.get('data') or {}).get('list') or []:
            title = item.get('title') or ''
            desc = item.get('desc') or ''
            views = (item.get('stat') or {}).get('view') or 0
            if title and is_finance(title + ' ' + desc):
                items.append({
                    'title': title[:80],
                    'summary': (desc[:200] or title),
                    'likes': safe_int(views),
                    'platform': 'Bç«™',
                    'source_type': 'æ’è¡Œ',
                    'sentiment': estimate_sentiment(title + ' ' + desc),
                    'creator_type': 'èšåˆçƒ­æ¦œ',
                    'publish_time': now_iso(),
                })
    except Exception as e:
        print(f'[Bç«™-æ’è¡Œ] {e}')

    return items

# ==================== å»é‡ ====================
def dedup(items):
    seen = set()
    result = []
    for item in items:
        key = re.sub(r'[\W\s]', '', (item.get('title') or ''))[:20]
        if not key or key in seen:
            continue
        seen.add(key)
        result.append(item)
    return result

# ==================== ä¸»é‡‡é›†æµç¨‹ ====================
ALL_FETCHERS = [
    ('æŠ–éŸ³', fetch_douyin),
    ('å¾®åš', fetch_weibo),
    ('ä¸œæ–¹è´¢å¯Œ', fetch_eastmoney),
    ('è´¢è”ç¤¾', fetch_cailian),
    ('çŸ¥ä¹', fetch_zhihu),
    ('ç™¾åº¦', fetch_baidu),
    ('Bç«™', fetch_bilibili),
]

def collect_all():
    """å¹¶è¡Œé‡‡é›†æ‰€æœ‰æ•°æ®æºï¼Œè¿”å› { items, source_counts, fetch_time, ... }"""
    all_items = []
    source_counts = {}
    errors = []

    print(f'[{datetime.now().strftime("%H:%M:%S")}] å¼€å§‹é‡‡é›† {len(ALL_FETCHERS)} ä¸ªæ•°æ®æº...')

    with ThreadPoolExecutor(max_workers=len(ALL_FETCHERS)) as executor:
        futures = {executor.submit(fn): name for name, fn in ALL_FETCHERS}
        for future in as_completed(futures):
            name = futures[future]
            try:
                items = future.result(timeout=20)
                source_counts[name] = len(items)
                all_items.extend(items)
                print(f'  âœ… {name}: {len(items)} æ¡')
            except Exception as e:
                source_counts[name] = 0
                errors.append(f'{name}: {str(e)}')
                print(f'  âŒ {name}: {e}')

    # å»é‡ + æ’åº
    all_items = dedup(all_items)
    all_items.sort(key=lambda x: x.get('likes', 0), reverse=True)

    result = {
        'items': all_items,
        'source_counts': source_counts,
        'total': len(all_items),
        'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'fetch_ts': int(time.time()),
        'errors': errors,
    }

    print(f'  ğŸ“Š å…±è®¡ {len(all_items)} æ¡ (å»é‡å)')
    return result

def save_cache(data):
    """å°†é‡‡é›†ç»“æœä¿å­˜åˆ° JSON ç¼“å­˜æ–‡ä»¶"""
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f'  ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {CACHE_FILE}')

def load_cache():
    """è¯»å–ç¼“å­˜æ–‡ä»¶"""
    if not os.path.exists(CACHE_FILE):
        return None
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None

def collect_and_save():
    """é‡‡é›†å¹¶ä¿å­˜ â€” ä¾› cron æˆ– server è°ƒç”¨"""
    data = collect_all()
    save_cache(data)
    return data

# ==================== CLI ====================
if __name__ == '__main__':
    data = collect_and_save()
    print(f'\né‡‡é›†å®Œæˆ: {data["total"]} æ¡, æ—¶é—´: {data["fetch_time"]}')
    for name, count in data['source_counts'].items():
        print(f'  {name}: {count}')
