#!/usr/bin/env python3
"""
ç¤¾äº¤åª’ä½“èˆ†æƒ…æ•°æ®æŠ“å–ç®¡é“
æŠ“å–æŠ–éŸ³çƒ­æ¦œ + å°çº¢ä¹¦çƒ­é—¨ç¬”è®° â†’ è¿‡æ»¤è´¢ç»ç›¸å…³ â†’ è¾“å‡º data/social_media_videos.json

æ•°æ®æµ: æŠ–éŸ³çƒ­æ¦œAPI + å°çº¢ä¹¦çƒ­ç‚¹ â†’ è´¢ç»å…³é”®è¯è¿‡æ»¤ â†’ ç»“æ„åŒ–è¾“å‡º â†’ å‰ç«¯æ¶ˆè´¹

å¯é€šè¿‡ GitHub Actions / cron å®šæ—¶è¿è¡Œï¼Œä¹Ÿå¯æ‰‹åŠ¨æ‰§è¡Œ
"""

import json, os, re, sys, ssl, time, hashlib
from datetime import datetime, timezone, timedelta
from urllib.request import urlopen, Request
from urllib.parse import quote, urlencode
from http.cookiejar import CookieJar
from urllib.request import build_opener, HTTPCookieProcessor, HTTPSHandler

# ==================== .env è‡ªåŠ¨åŠ è½½ ====================
def _load_dotenv():
    env_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
    if not os.path.exists(env_path):
        return
    with open(env_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#') or '=' not in line:
                continue
            key, _, value = line.partition('=')
            key, value = key.strip(), value.strip()
            if key and key not in os.environ:
                os.environ[key] = value

_load_dotenv()

# ==================== é…ç½® ====================
OUTPUT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'social_media_videos.json')

# è´¢ç»ç›¸å…³å…³é”®è¯ (ç”¨äºè¿‡æ»¤éè´¢ç»å†…å®¹)
FINANCE_KEYWORDS = [
    # å¤§ç›˜/æŒ‡æ•°
    'Aè‚¡', 'è‚¡å¸‚', 'å¤§ç›˜', 'æ²ªæŒ‡', 'ä¸Šè¯', 'æ·±æˆ', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿', 'åŒ—è¯',
    'æ²ªæ·±300', 'ä¸­è¯500', 'ä¸­è¯1000', 'æ’ç”Ÿ', 'æ¸¯è‚¡', 'ç¾è‚¡', 'çº³æ–¯è¾¾å…‹', 'é“ç¼æ–¯',
    # è¡Œä¸š/æ¿å—
    'AI', 'äººå·¥æ™ºèƒ½', 'ç®—åŠ›', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'å…‰æ¨¡å—', 'CPO', 'å¤§æ¨¡å‹', 'DeepSeek',
    'æœºå™¨äºº', 'è‡ªåŠ¨é©¾é©¶', 'æ–°èƒ½æº', 'å…‰ä¼', 'é”‚ç”µ', 'ç¢³é…¸é”‚', 'å‚¨èƒ½',
    'å†›å·¥', 'å›½é˜²', 'èˆªå¤©', 'ç™½é…’', 'æ¶ˆè´¹', 'åŒ»è¯', 'åˆ›æ–°è¯', 'CXO',
    'é»„é‡‘', 'é‡‘ä»·', 'åŸæ²¹', 'æ²¹ä»·', 'æœ‰è‰²é‡‘å±', 'é“œ', 'é“', 'ç¨€åœŸ',
    'çº¢åˆ©', 'é«˜è‚¡æ¯', 'é“¶è¡Œ', 'ä¿é™©', 'åˆ¸å•†', 'åœ°äº§', 'æˆ¿åœ°äº§',
    # å®è§‚/æ”¿ç­–
    'å¤®è¡Œ', 'é™æ¯', 'é™å‡†', 'LPR', 'åˆ©ç‡', 'é€šèƒ€', 'CPI', 'PPI', 'GDP', 'PMI',
    'ç¾è”å‚¨', 'Fed', 'åŠ æ¯', 'ç¼©è¡¨', 'å›½å€º', 'å€ºåˆ¸', 'æ±‡ç‡', 'äººæ°‘å¸', 'ç¾å…ƒ',
    'å…³ç¨', 'è´¸æ˜“æˆ˜', 'åˆ¶è£', 'åœ°ç¼˜', 'ä¸­ä¸œ', 'ä¿„ä¹Œ', 'ç¾ä¼Š',
    # æŠ•èµ„/ç†è´¢
    'åŸºé‡‘', 'ETF', 'ç‰›å¸‚', 'ç†Šå¸‚', 'æ¶¨åœ', 'è·Œåœ', 'æŠ„åº•', 'è¿½é«˜', 'å‰²è‚‰',
    'ä»“ä½', 'åŠ ä»“', 'å‡ä»“', 'æ¸…ä»“', 'æ»¡ä»“', 'ç©ºä»“', 'å®šæŠ•',
    'ä¸»åŠ›', 'èµ„é‡‘', 'åŒ—å‘', 'èèµ„', 'èåˆ¸', 'æ æ†',
    'èŒ…å°', 'æ¯”äºšè¿ª', 'å®å¾·', 'è‹±ä¼Ÿè¾¾', 'NVIDIA', 'ç‰¹æ–¯æ‹‰',
    'IPO', 'åˆ†çº¢', 'å›è´­', 'å¹¶è´­', 'é‡ç»„',
]

# éœ€è¦è¿‡æ»¤çš„è¥é”€/åšçœ¼çƒå…³é”®è¯
NOISE_KEYWORDS = [
    'éœ‡æƒŠ', 'ä¸è½¬ä¸æ˜¯ä¸­å›½äºº', 'é€Ÿçœ‹', 'å¿…çœ‹', 'åˆ å‰å¿«çœ‹',
    'æœ€åä¸€æ¬¡æœºä¼š', 'å…¨ä»“æ¢­å“ˆ', 'æ™šäº†å°±æ¥ä¸åŠ', 'èµ¶ç´§',
]

# ç”¨æˆ·ä»£ç†
UA_MOBILE = 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
UA_DESKTOP = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'

NOW = datetime.now(timezone(timedelta(hours=8)))


def _ssl_ctx():
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    return ctx


def fetch_url(url, headers=None, timeout=15):
    """é€šç”¨ HTTP GET"""
    h = {
        'User-Agent': UA_DESKTOP,
        'Accept': 'application/json, text/html, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    if headers:
        h.update(headers)
    req = Request(url, headers=h)
    try:
        with urlopen(req, timeout=timeout, context=_ssl_ctx()) as resp:
            return resp.read().decode('utf-8', errors='replace')
    except Exception as e:
        print(f"  [WARN] fetch failed: {url[:80]}... - {e}", file=sys.stderr)
        return None


def is_finance_related(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸è´¢ç»ç›¸å…³"""
    if not text:
        return False
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in FINANCE_KEYWORDS)


def noise_score(text):
    """è®¡ç®—å™ªéŸ³åˆ† (è¶Šé«˜è¶Šå¯èƒ½æ˜¯è¥é”€/åšçœ¼çƒ)"""
    if not text:
        return 0
    score = 0
    for kw in NOISE_KEYWORDS:
        if kw in text:
            score += 1
    # æ„Ÿå¹å·æ•°é‡
    score += text.count('ï¼') * 0.3
    score += text.count('!') * 0.3
    # å…¨å¤§å†™æ ‡è¯†
    if text == text.upper() and len(text) > 5:
        score += 1
    return score


def gen_id(text):
    """ç”Ÿæˆç¡®å®šæ€§å”¯ä¸€ ID"""
    return 'sm_' + hashlib.md5(text.encode()).hexdigest()[:12]


# ==================== æŠ–éŸ³çƒ­æ¦œæŠ“å– ====================

def fetch_douyin_hot():
    """
    æŠ–éŸ³çƒ­æ¦œ (å…¬å¼€ Web API)
    è¿”å›çƒ­æœè¯æ¡åˆ—è¡¨ â†’ è¿‡æ»¤è´¢ç»ç›¸å…³
    """
    print("  ğŸ“± æŠ–éŸ³çƒ­æ¦œæŠ“å–...")
    items = []

    # æ–¹å¼1: æŠ–éŸ³çƒ­ç‚¹æ¦œ API
    hot_urls = [
        'https://www.douyin.com/aweme/v1/web/hot/search/list/',
        'https://www.iesdouyin.com/web/api/v2/hotsearch/billboard/word/',
    ]

    for url in hot_urls:
        raw = fetch_url(url, headers={
            'User-Agent': UA_DESKTOP,
            'Referer': 'https://www.douyin.com/',
        })
        if not raw:
            continue
        try:
            data = json.loads(raw)
            word_list = (data.get('data', {}).get('word_list', []) or
                        data.get('word_list', []) or [])
            for item in word_list:
                word = item.get('word', '') or item.get('query', '') or ''
                hot_value = item.get('hot_value', 0) or item.get('search_count', 0) or 0
                event_time = item.get('event_time', '') or ''
                if word and is_finance_related(word):
                    items.append({
                        'title': word,
                        'summary': item.get('word_sub_board', '') or item.get('label', '') or '',
                        'likes': int(hot_value) if hot_value else 0,
                        'platform': 'æŠ–éŸ³',
                        'source_type': 'çƒ­æ¦œ',
                    })
            if items:
                break
        except Exception as e:
            print(f"    [WARN] æŠ–éŸ³çƒ­æ¦œè§£æå¤±è´¥: {e}", file=sys.stderr)

    # æ–¹å¼2: ç¬¬ä¸‰æ–¹èšåˆ (tophub)
    if not items:
        raw = fetch_url('https://api.vvhan.com/api/hotlist/douyinHot', headers={
            'User-Agent': UA_DESKTOP,
        })
        if raw:
            try:
                data = json.loads(raw)
                for item in (data.get('data', []) or []):
                    title = item.get('title', '') or item.get('name', '') or ''
                    hot = item.get('hot', 0) or item.get('hotValue', 0) or 0
                    if title and is_finance_related(title):
                        items.append({
                            'title': title,
                            'summary': item.get('desc', '') or '',
                            'likes': int(hot) if hot else 0,
                            'platform': 'æŠ–éŸ³',
                            'source_type': 'çƒ­æœ',
                        })
            except Exception as e:
                print(f"    [WARN] æŠ–éŸ³å¤‡ç”¨æºå¤±è´¥: {e}", file=sys.stderr)

    # æ–¹å¼3: éŸ©å°éŸ©çƒ­æ¦œAPI
    if not items:
        raw = fetch_url('https://api.vvhan.com/api/hotlist?type=douyinHot')
        if raw:
            try:
                data = json.loads(raw)
                for item in (data.get('data', []) or []):
                    title = item.get('title', '') or ''
                    hot = item.get('hot', 0) or 0
                    if title and is_finance_related(title):
                        items.append({
                            'title': title,
                            'summary': item.get('desc', '') or '',
                            'likes': int(hot) if hot else 0,
                            'platform': 'æŠ–éŸ³',
                            'source_type': 'çƒ­æœ',
                        })
            except Exception as e:
                print(f"    [WARN] æŠ–éŸ³éŸ©å°éŸ©APIå¤±è´¥: {e}", file=sys.stderr)

    print(f"    âœ… æŠ–éŸ³è´¢ç»ç›¸å…³: {len(items)} æ¡")
    return items


def fetch_douyin_finance_videos():
    """
    æŠ–éŸ³è´¢ç»ç±»è§†é¢‘çƒ­é—¨å†…å®¹ (æœç´¢API / æ¨èæµ)
    æœç´¢é¢„è®¾è´¢ç»å…³é”®è¯è·å–çƒ­é—¨è§†é¢‘
    """
    print("  ğŸ¬ æŠ–éŸ³è´¢ç»è§†é¢‘æœç´¢...")
    items = []
    search_keywords = ['AIç®—åŠ›', 'è‚¡å¸‚', 'åŸºé‡‘', 'é»„é‡‘æŠ•èµ„', 'åŠå¯¼ä½“', 'æ–°èƒ½æº', 'å†›å·¥', 'æ¸¯è‚¡']

    for kw in search_keywords:
        # 60 second API
        url = f'https://www.douyin.com/aweme/v1/web/general/search/single/?keyword={quote(kw)}&search_channel=aweme_general&sort_type=2&publish_time=1&count=5'
        raw = fetch_url(url, headers={
            'User-Agent': UA_DESKTOP,
            'Referer': f'https://www.douyin.com/search/{quote(kw)}',
            'Cookie': 'ttwid=placeholder',
        })
        if not raw:
            continue
        try:
            data = json.loads(raw)
            aweme_list = data.get('data', []) or data.get('aweme_list', []) or []
            for aweme in aweme_list[:3]:
                desc = aweme.get('aweme_info', aweme).get('desc', '') or ''
                stats = aweme.get('aweme_info', aweme).get('statistics', {})
                if desc and is_finance_related(desc):
                    items.append({
                        'title': desc[:80],
                        'summary': desc,
                        'likes': stats.get('digg_count', 0) or 0,
                        'shares': stats.get('share_count', 0) or 0,
                        'comments_count': stats.get('comment_count', 0) or 0,
                        'platform': 'æŠ–éŸ³',
                        'source_type': 'è§†é¢‘æœç´¢',
                    })
        except Exception as e:
            print(f"    [WARN] æŠ–éŸ³æœç´¢ '{kw}' å¤±è´¥: {e}", file=sys.stderr)
        time.sleep(0.5)  # é¿å…é¢‘ç‡é™åˆ¶

    print(f"    âœ… æŠ–éŸ³è§†é¢‘æœç´¢: {len(items)} æ¡")
    return items


# ==================== å°çº¢ä¹¦çƒ­ç‚¹æŠ“å– ====================

def fetch_xiaohongshu_hot():
    """
    å°çº¢ä¹¦çƒ­é—¨è¯é¢˜/ç¬”è®° (å…¬å¼€æ¥å£)
    """
    print("  ğŸ“• å°çº¢ä¹¦çƒ­ç‚¹æŠ“å–...")
    items = []

    # ç¬¬ä¸‰æ–¹çƒ­æ¦œèšåˆ
    hot_urls = [
        'https://api.vvhan.com/api/hotlist/xhsHot',
        'https://api.vvhan.com/api/hotlist?type=xiaohongshuHot',
    ]

    for url in hot_urls:
        raw = fetch_url(url, headers={'User-Agent': UA_DESKTOP})
        if not raw:
            continue
        try:
            data = json.loads(raw)
            data_list = data.get('data', []) or []
            for item in data_list:
                title = item.get('title', '') or item.get('name', '') or ''
                hot = item.get('hot', 0) or item.get('hotValue', 0) or 0
                desc = item.get('desc', '') or item.get('description', '') or ''
                if title and is_finance_related(title + desc):
                    items.append({
                        'title': title,
                        'summary': desc,
                        'likes': int(hot) if hot else 0,
                        'platform': 'å°çº¢ä¹¦',
                        'source_type': 'çƒ­æœ',
                    })
            if items:
                break
        except Exception as e:
            print(f"    [WARN] å°çº¢ä¹¦çƒ­æ¦œè§£æå¤±è´¥: {e}", file=sys.stderr)

    print(f"    âœ… å°çº¢ä¹¦è´¢ç»ç›¸å…³: {len(items)} æ¡")
    return items


def fetch_xiaohongshu_finance_notes():
    """
    å°çº¢ä¹¦è´¢ç»ç¬”è®°æœç´¢ (å…¬å¼€é¡µé¢æå–)
    """
    print("  ğŸ“ å°çº¢ä¹¦è´¢ç»ç¬”è®°æœç´¢...")
    items = []
    search_keywords = ['åŸºé‡‘æ¨è', 'AIç®—åŠ›æŠ•èµ„', 'é»„é‡‘è¿˜èƒ½ä¹°å—', 'æ–°èƒ½æºåŸºé‡‘', 'æ¶ˆè´¹åŸºé‡‘', 'æ¸¯è‚¡ETF']

    for kw in search_keywords:
        url = f'https://www.xiaohongshu.com/search_result?keyword={quote(kw)}&type=51'
        raw = fetch_url(url, headers={
            'User-Agent': UA_DESKTOP,
            'Referer': 'https://www.xiaohongshu.com/',
        })
        if not raw:
            continue
        try:
            # å°è¯•ä»HTMLä¸­æå–åˆå§‹åŒ–æ•°æ®
            json_match = re.search(r'window\.__INITIAL_STATE__\s*=\s*(\{.*?\})\s*</script>', raw, re.DOTALL)
            if json_match:
                # å°çº¢ä¹¦ä½¿ç”¨ undefined æ›¿ä»£, éœ€è¦æ›¿æ¢
                raw_json = json_match.group(1).replace('undefined', 'null')
                data = json.loads(raw_json)
                notes = data.get('search', {}).get('notes', {}).get('items', []) or []
                for note in notes[:3]:
                    note_data = note.get('noteCard', note.get('note', {}))
                    title = note_data.get('title', '') or note_data.get('displayTitle', '') or ''
                    desc = note_data.get('desc', '') or ''
                    liked = note_data.get('interactInfo', {}).get('likedCount', 0) or 0
                    if title:
                        items.append({
                            'title': title[:80],
                            'summary': desc[:200] if desc else title,
                            'likes': int(liked) if liked else 0,
                            'platform': 'å°çº¢ä¹¦',
                            'source_type': 'ç¬”è®°æœç´¢',
                        })
        except Exception as e:
            print(f"    [WARN] å°çº¢ä¹¦æœç´¢ '{kw}' å¤±è´¥: {e}", file=sys.stderr)
        time.sleep(0.5)

    print(f"    âœ… å°çº¢ä¹¦ç¬”è®°æœç´¢: {len(items)} æ¡")
    return items


# ==================== å¾®åšè´¢ç»çƒ­æœ (è¡¥å……æº) ====================

def fetch_weibo_finance_hot():
    """å¾®åšè´¢ç»çƒ­æœ (çº¯å…¬å¼€API, è¡¥å……ç¤¾äº¤åª’ä½“ç»´åº¦)"""
    print("  ğŸ¦ å¾®åšè´¢ç»çƒ­æœ...")
    items = []

    # å¾®åšçƒ­æœAPI
    urls = [
        'https://weibo.com/ajax/side/hotSearch',
        'https://api.vvhan.com/api/hotlist/wbHot',
    ]

    for url in urls:
        raw = fetch_url(url, headers={
            'User-Agent': UA_DESKTOP,
            'Referer': 'https://weibo.com/',
        })
        if not raw:
            continue
        try:
            data = json.loads(raw)
            # å¾®åšå®˜æ–¹APIæ ¼å¼
            realtime = data.get('data', {}).get('realtime', []) or []
            if realtime:
                for item in realtime:
                    word = item.get('word', '') or item.get('note', '') or ''
                    num = item.get('num', 0) or item.get('raw_hot', 0) or 0
                    label_name = item.get('label_name', '') or ''
                    if word and is_finance_related(word):
                        items.append({
                            'title': word,
                            'summary': label_name,
                            'likes': int(num) if num else 0,
                            'platform': 'å¾®åš',
                            'source_type': 'çƒ­æœ',
                        })
                if items:
                    break

            # ç¬¬ä¸‰æ–¹APIæ ¼å¼
            data_list = data.get('data', []) if isinstance(data.get('data'), list) else []
            for item in data_list:
                title = item.get('title', '') or item.get('name', '') or ''
                hot = item.get('hot', 0) or item.get('hotValue', 0) or 0
                if title and is_finance_related(title):
                    items.append({
                        'title': title,
                        'summary': item.get('desc', '') or '',
                        'likes': int(hot) if hot else 0,
                        'platform': 'å¾®åš',
                        'source_type': 'çƒ­æœ',
                    })
            if items:
                break
        except Exception as e:
            print(f"    [WARN] å¾®åšçƒ­æœè§£æå¤±è´¥: {e}", file=sys.stderr)

    print(f"    âœ… å¾®åšè´¢ç»ç›¸å…³: {len(items)} æ¡")
    return items


# ==================== ä¸œæ–¹è´¢å¯Œ/åŒèŠ±é¡ºç¤¾åŒºèˆ†æƒ… ====================

def fetch_eastmoney_community():
    """ä¸œæ–¹è´¢å¯Œè‚¡å§çƒ­é—¨è¯é¢˜ (çº¯è´¢ç»ç¤¾åŒº)"""
    print("  ğŸ’¬ ä¸œæ–¹è´¢å¯Œè‚¡å§çƒ­å¸–...")
    items = []

    # è‚¡å§çƒ­å¸–
    urls = [
        'https://guba.eastmoney.com/interface/GetData.aspx?path=newtopic/api/Topic/HomePageListRead&param=ps%3D30%26p%3D1',
        'https://gbapi.eastmoney.com/senti/api/Topic/GetHotTopicList?ps=30&p=1',
    ]

    # ä¸œæ–¹è´¢å¯Œ7x24äººæ°”æ¦œ
    raw = fetch_url('https://np-listapi.eastmoney.com/comm/web/getNewsByColumns?type=0&client=web&maxNewsId=0&pageSize=30&column=102')
    if raw:
        try:
            data = json.loads(raw)
            data_obj = data.get('data') or {}
            if isinstance(data_obj, dict):
                news_list = data_obj.get('list', []) or []
            else:
                news_list = []
            for item in news_list:
                title = (item.get('title') or '').strip()
                content = (item.get('content') or '').strip()
                text = title or content[:100]
                if text and len(text) >= 6 and is_finance_related(text):
                    items.append({
                        'title': text[:80],
                        'summary': content[:200] if content else text,
                        'likes': 0,
                        'platform': 'ä¸œæ–¹è´¢å¯Œ',
                        'source_type': 'å¿«è®¯',
                    })
        except Exception as e:
            print(f"    [WARN] ä¸œæ–¹è´¢å¯Œè§£æå¤±è´¥: {e}", file=sys.stderr)

    print(f"    âœ… ä¸œæ–¹è´¢å¯Œ: {len(items)} æ¡")
    return items


# ==================== ç»¼åˆè´¢ç»ç¤¾äº¤çƒ­ç‚¹ (å…œåº•) ====================

def fetch_tophub_finance():
    """
    ä»Šæ—¥çƒ­æ¦œèšåˆ - è´¢ç»ç›¸å…³å¹³å°
    è¦†ç›–: 36æ°ªã€è™å—…ã€è´¢è”ç¤¾ç­‰
    """
    print("  ğŸ”¥ èšåˆè´¢ç»çƒ­æ¦œ...")
    items = []

    tophub_sources = [
        ('https://api.vvhan.com/api/hotlist/36Ke', '36æ°ª'),
        ('https://api.vvhan.com/api/hotlist/huXiu', 'è™å—…'),
        ('https://api.vvhan.com/api/hotlist/zhihuHot', 'çŸ¥ä¹'),
        ('https://api.vvhan.com/api/hotlist/baiduRD', 'ç™¾åº¦'),
        ('https://api.vvhan.com/api/hotlist/bili', 'Bç«™'),
    ]

    for url, platform in tophub_sources:
        raw = fetch_url(url, headers={'User-Agent': UA_DESKTOP})
        if not raw:
            continue
        try:
            data = json.loads(raw)
            data_list = data.get('data', []) or []
            for item in data_list:
                title = item.get('title', '') or item.get('name', '') or ''
                hot = item.get('hot', 0) or item.get('hotValue', 0) or 0
                desc = item.get('desc', '') or ''
                if title and is_finance_related(title + desc):
                    items.append({
                        'title': title[:80],
                        'summary': desc[:200] if desc else '',
                        'likes': int(hot) if hot else 0,
                        'platform': platform,
                        'source_type': 'çƒ­æ¦œ',
                    })
        except Exception as e:
            print(f"    [WARN] {platform} è§£æå¤±è´¥: {e}", file=sys.stderr)
        time.sleep(0.3)

    print(f"    âœ… èšåˆçƒ­æ¦œè´¢ç»ç›¸å…³: {len(items)} æ¡")
    return items


# ==================== æ•°æ®å¤„ç† ====================

def deduplicate(items):
    """å»é‡ (åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦)"""
    seen = set()
    unique = []
    for item in items:
        # ç®€å•å»é‡: å–æ ‡é¢˜å‰20å­—
        key = re.sub(r'\W', '', item['title'])[:20]
        if key and key not in seen:
            seen.add(key)
            unique.append(item)
    return unique


def estimate_sentiment(item):
    """åŸºäºæ ‡é¢˜å…³é”®è¯ä¼°ç®—æƒ…ç»ªæ ‡ç­¾"""
    title = item.get('title', '') + ' ' + item.get('summary', '')

    # æåº¦çœ‹å¤šä¿¡å·
    if re.search(r'æš´æ¶¨|ç–¯æ¶¨|å¤§æ¶¨|é£™å‡|æ¶¨åœ|å…¨ä»“|æ¢­å“ˆ|èµ·é£|çˆ†å‘|ç‰›å¸‚|åˆ›æ–°é«˜', title):
        return 'æåº¦çœ‹å¤š'
    # çœ‹å¤š
    if re.search(r'ä¸Šæ¶¨|èµ°é«˜|åå¼¹|åˆ©å¥½|åŠ ä»“|æœºä¼š|çªç ´|çœ‹å¥½|æ¨è|é…ç½®', title):
        return 'åå¤š'
    # æåº¦çœ‹ç©º
    if re.search(r'æš´è·Œ|å´©ç›˜|å¤§è·Œ|è·³æ°´|æ¸…ä»“|å‰²è‚‰|ç†Šå¸‚|è…°æ–©|å´©', title):
        return 'æåº¦æ‚²è§‚'
    # çœ‹ç©º
    if re.search(r'ä¸‹è·Œ|èµ°ä½|åˆ©ç©º|å‡ä»“|é£é™©|è­¦æƒ•|è°¨æ…|å›è°ƒ|æ‰¿å‹', title):
        return 'åç©º'
    # ä¸­æ€§
    if re.search(r'éœ‡è¡|åˆ†æ­§|è§‚æœ›|æŒå¹³|ç¨³å®š|ç›˜æ•´', title):
        return 'ä¸­æ€§'

    return 'ä¸­æ€§åå¤š'


def estimate_creator_type(item):
    """ä¼°ç®—å†…å®¹æ¥æºç±»å‹"""
    platform = item.get('platform', '')
    source_type = item.get('source_type', '')

    if platform in ['ä¸œæ–¹è´¢å¯Œ', 'è´¢è”ç¤¾']:
        return 'è´¢ç»èµ„è®¯å¹³å°'
    if source_type == 'çƒ­æœ':
        return 'ç¤¾äº¤åª’ä½“çƒ­æœ'
    if source_type == 'è§†é¢‘æœç´¢':
        return 'çŸ­è§†é¢‘åˆ›ä½œè€…'
    if source_type == 'ç¬”è®°æœç´¢':
        return 'ç†è´¢åšä¸»'
    if platform == '36æ°ª':
        return 'ç§‘æŠ€è´¢ç»åª’ä½“'
    if platform == 'è™å—…':
        return 'å•†ä¸šåˆ†æå¸ˆ'
    if platform == 'çŸ¥ä¹':
        return 'çŸ¥è¯†ç¤¾åŒºç”¨æˆ·'
    if platform == 'Bç«™':
        return 'UPä¸»/è§†é¢‘åˆ›ä½œè€…'
    return 'è´¢ç»åšä¸»'


def process_items(all_items):
    """å¤„ç†æ‰€æœ‰æŠ“å–ç»“æœ, ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
    # å»é‡
    unique = deduplicate(all_items)

    # æŒ‰çƒ­åº¦æ’åº
    unique.sort(key=lambda x: x.get('likes', 0), reverse=True)

    # è¿‡æ»¤å™ªéŸ³
    filtered = []
    for item in unique:
        ns = noise_score(item.get('title', ''))
        if ns >= 2:
            item['noise_flag'] = True
        filtered.append(item)

    # æ·»åŠ ä¼°ç®—å­—æ®µ
    result = []
    for i, item in enumerate(filtered[:50]):  # æœ€å¤š50æ¡
        entry = {
            'id': gen_id(item['title'] + str(i)),
            'platform': item.get('platform', 'æœªçŸ¥'),
            'title': item.get('title', ''),
            'summary': item.get('summary', '') or item.get('title', ''),
            'likes': item.get('likes', 0),
            'shares': item.get('shares', 0),
            'comments_count': item.get('comments_count', 0),
            'sentiment': estimate_sentiment(item),
            'main_opinion': item.get('summary', '')[:50] if item.get('summary') else '',
            'creator_type': estimate_creator_type(item),
            'source_type': item.get('source_type', ''),
            'publish_time': NOW.isoformat(),
            'noise_flag': item.get('noise_flag', False),
        }
        result.append(entry)

    return result


# ==================== ä¸»æµç¨‹ ====================

def main():
    print(f"\n{'='*60}")
    print(f"ğŸ“¡ ç¤¾äº¤åª’ä½“èˆ†æƒ…æŠ“å– - {NOW.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    all_items = []

    # 1. æŠ–éŸ³çƒ­æ¦œ
    try:
        all_items.extend(fetch_douyin_hot())
    except Exception as e:
        print(f"  âŒ æŠ–éŸ³çƒ­æ¦œå¼‚å¸¸: {e}", file=sys.stderr)

    # 2. æŠ–éŸ³è´¢ç»è§†é¢‘æœç´¢
    try:
        all_items.extend(fetch_douyin_finance_videos())
    except Exception as e:
        print(f"  âŒ æŠ–éŸ³è§†é¢‘æœç´¢å¼‚å¸¸: {e}", file=sys.stderr)

    # 3. å°çº¢ä¹¦çƒ­ç‚¹
    try:
        all_items.extend(fetch_xiaohongshu_hot())
    except Exception as e:
        print(f"  âŒ å°çº¢ä¹¦çƒ­ç‚¹å¼‚å¸¸: {e}", file=sys.stderr)

    # 4. å°çº¢ä¹¦è´¢ç»ç¬”è®°
    try:
        all_items.extend(fetch_xiaohongshu_finance_notes())
    except Exception as e:
        print(f"  âŒ å°çº¢ä¹¦ç¬”è®°å¼‚å¸¸: {e}", file=sys.stderr)

    # 5. å¾®åšè´¢ç»çƒ­æœ
    try:
        all_items.extend(fetch_weibo_finance_hot())
    except Exception as e:
        print(f"  âŒ å¾®åšçƒ­æœå¼‚å¸¸: {e}", file=sys.stderr)

    # 6. ä¸œæ–¹è´¢å¯Œç¤¾åŒº
    try:
        all_items.extend(fetch_eastmoney_community())
    except Exception as e:
        print(f"  âŒ ä¸œæ–¹è´¢å¯Œå¼‚å¸¸: {e}", file=sys.stderr)

    # 7. èšåˆè´¢ç»çƒ­æ¦œ (36æ°ª/è™å—…/çŸ¥ä¹/ç™¾åº¦/Bç«™)
    try:
        all_items.extend(fetch_tophub_finance())
    except Exception as e:
        print(f"  âŒ èšåˆçƒ­æ¦œå¼‚å¸¸: {e}", file=sys.stderr)

    print(f"\n  ğŸ“Š æ€»æŠ“å–: {len(all_items)} æ¡")

    # å¤„ç†
    processed = process_items(all_items)

    print(f"  ğŸ“‹ å»é‡+è¿‡æ»¤å: {len(processed)} æ¡")

    # è¾“å‡º
    output = {
        'updated_at': NOW.isoformat(),
        'total_fetched': len(all_items),
        'total_processed': len(processed),
        'sources': list(set(item.get('platform', '') for item in processed)),
        'videos': processed,
    }

    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\n  âœ… è¾“å‡ºè‡³: {OUTPUT_PATH}")
    print(f"  ğŸ“± å¹³å°è¦†ç›–: {', '.join(output['sources'])}")
    print(f"{'='*60}\n")

    return output


if __name__ == '__main__':
    main()
