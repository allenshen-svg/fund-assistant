#!/usr/bin/env python3
"""
åŸºé‡‘åŠ©æ‰‹ - å®è§‚äº‹ä»¶è‡ªåŠ¨è¿½è¸ªç®¡é“
GitHub Actions æ¯2å°æ—¶è¿è¡Œ: æŠ“å–è´¢ç»æ–°é—» â†’ LLMç»“æ„åŒ–æå– â†’ è¾“å‡º data/hot_events.json

æ•°æ®æµ: æ–°é—»æº â†’ AIäº‹ä»¶æç‚¼ â†’ æ¦‚å¿µæ ‡ç­¾ â†’ è¡Œä¸šæ˜ å°„ â†’ hot_events.json â†’ å‰ç«¯æ¶ˆè´¹
"""

import json, os, re, sys, ssl, time
from datetime import datetime, timezone, timedelta
from urllib.request import urlopen, Request
from http.cookiejar import CookieJar
from urllib.request import build_opener, HTTPCookieProcessor, HTTPSHandler

# ==================== .env è‡ªåŠ¨åŠ è½½ ====================
def _load_dotenv():
    """ä»é¡¹ç›®æ ¹ç›®å½• .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä¸è¦†ç›–å·²æœ‰å˜é‡ï¼‰"""
    env_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
    if not os.path.exists(env_path):
        return
    with open(env_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#') or '=' not in line:
                continue
            key, _, value = line.partition('=')
            key, value = key.strip(), value.strip()
            if key and key not in os.environ:
                os.environ[key] = value

_load_dotenv()

# ==================== é…ç½® ====================
API_KEY = os.environ.get('AI_API_KEY', '')
API_BASE = os.environ.get('AI_API_BASE', 'https://api.siliconflow.cn/v1')
MODEL = os.environ.get('AI_MODEL', 'deepseek-ai/DeepSeek-V3')
OUTPUT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'hot_events.json')
SENTIMENT_CACHE_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'sentiment_cache.json')
ANALYSIS_CACHE_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'analysis_cache.json')
XUEQIU_COOKIE = os.environ.get('XUEQIU_COOKIE', '').strip()

# 20ä¸ªæ ¸å¿ƒå¸‚åœºæ ‡ç­¾ (å‰ç«¯fundæ ‡ç­¾ä½“ç³»å¯¹é½)
MARKET_TAGS = [
    'äººå·¥æ™ºèƒ½', 'AIç®—åŠ›', 'åŠå¯¼ä½“', 'æœºå™¨äºº', 'å¤§æ¨¡å‹',
    'æ–°èƒ½æº', 'å…‰ä¼', 'é”‚ç”µ', 'æ–°èƒ½æºè½¦',
    'ç™½é…’', 'æ¶ˆè´¹', 'åŒ»è¯', 'æ¸¯è‚¡ç§‘æŠ€',
    'é»„é‡‘', 'æœ‰è‰²é‡‘å±', 'åŸæ²¹', 'å†›å·¥',
    'çº¢åˆ©', 'å€ºåˆ¸', 'å®½åŸº',
]

# æ ‡ç­¾ â†’ è¡Œä¸šæ¿å—æ˜ å°„ (ä¸å‰ç«¯ FUND_SECTOR_KEYWORD_MAP å¯¹é½)
TAG_TO_SECTORS = {
    'äººå·¥æ™ºèƒ½': ['AI/ç§‘æŠ€', 'ç§‘æŠ€', 'AIGC'],
    'AIç®—åŠ›': ['AI/ç§‘æŠ€', 'åŠå¯¼ä½“', 'ç®—åŠ›'],
    'åŠå¯¼ä½“': ['åŠå¯¼ä½“', 'èŠ¯ç‰‡', 'ç§‘æŠ€'],
    'æœºå™¨äºº': ['æœºå™¨äºº', 'AI/ç§‘æŠ€', 'ç§‘æŠ€'],
    'å¤§æ¨¡å‹': ['AI/ç§‘æŠ€', 'AIGC', 'ç§‘æŠ€'],
    'æ–°èƒ½æº': ['æ–°èƒ½æº', 'å…‰ä¼', 'é”‚ç”µ'],
    'å…‰ä¼': ['å…‰ä¼', 'æ–°èƒ½æº'],
    'é”‚ç”µ': ['é”‚ç”µ', 'æ–°èƒ½æº', 'ç”µæ± '],
    'æ–°èƒ½æºè½¦': ['æ–°èƒ½æºè½¦', 'æ–°èƒ½æº'],
    'ç™½é…’': ['ç™½é…’', 'æ¶ˆè´¹', 'é£Ÿå“é¥®æ–™'],
    'æ¶ˆè´¹': ['æ¶ˆè´¹', 'é£Ÿå“é¥®æ–™', 'å†…éœ€'],
    'åŒ»è¯': ['åŒ»è¯', 'åˆ›æ–°è¯', 'ç”Ÿç‰©åŒ»è¯'],
    'æ¸¯è‚¡ç§‘æŠ€': ['æ¸¯è‚¡ç§‘æŠ€', 'æ¸¯è‚¡äº’è”ç½‘', 'æ’ç”Ÿç§‘æŠ€', 'QDIIç§‘æŠ€'],
    'é»„é‡‘': ['é»„é‡‘', 'è´µé‡‘å±'],
    'æœ‰è‰²é‡‘å±': ['æœ‰è‰²é‡‘å±', 'é“œé“', 'å¤§å®—å•†å“'],
    'åŸæ²¹': ['åŸæ²¹', 'èƒ½æº', 'æ²¹æ°”'],
    'å†›å·¥': ['å†›å·¥', 'å›½é˜²', 'èˆªå¤©'],
    'çº¢åˆ©': ['çº¢åˆ©', 'é«˜è‚¡æ¯', 'ä½æ³¢åŠ¨'],
    'å€ºåˆ¸': ['å€ºåˆ¸', 'å›ºæ”¶', 'çº¯å€º'],
    'å®½åŸº': ['å®½åŸº', 'æ²ªæ·±300', 'ä¸­è¯500'],
}

CATEGORY_ICONS = {
    'technology': 'ğŸ¤–', 'geopolitics': 'ğŸŒ', 'monetary': 'ğŸ¦',
    'policy': 'ğŸ“œ', 'commodity': 'ğŸ›¢ï¸', 'market': 'ğŸ“Š',
}

_ANALYST_HINT_WORDS = ['åˆ†æå¸ˆ', 'é¦–å¸­', 'åŸºé‡‘ç»ç†', 'ç­–ç•¥', 'ç ”æŠ¥', 'è§‚ç‚¹', 'è§£è¯»', 'çœ‹å¤š', 'çœ‹ç©º', 'å»ºè®®']


def _safe_read_json(path):
    try:
        if not os.path.exists(path):
            return None
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None


def _clean_text(text, max_len=140):
    t = re.sub(r'\s+', ' ', str(text or '')).strip()
    return t[:max_len]


def _extract_analyst_views(max_items=16):
    """ä» analysis_cache + sentiment_cache æŠ½å–çƒ­é—¨åˆ†æå¸ˆè§‚ç‚¹"""
    views = []

    analysis_cache = _safe_read_json(ANALYSIS_CACHE_PATH) or {}
    for sec in (analysis_cache.get('kol_sections') or [])[:8]:
        target = _clean_text(sec.get('target', ''), 32)
        kol = _clean_text(sec.get('kol', ''), 180)
        if not (target and kol):
            continue
        views.append({
            'target': target,
            'text': f"{target}: {kol}",
            'likes': 100000,
            'source': 'KOLåˆ†æç¼“å­˜',
        })

    sentiment_cache = _safe_read_json(SENTIMENT_CACHE_PATH) or {}
    for item in (sentiment_cache.get('items') or []):
        title = _clean_text(item.get('title', ''), 90)
        summary = _clean_text(item.get('summary', ''), 110)
        creator_type = _clean_text(item.get('creator_type', ''), 24)
        text_join = f"{title} {summary}"
        if not title:
            continue
        if creator_type not in ['è´¢ç»é¢‘é“', 'è´¢ç»èµ„è®¯å¹³å°', 'è§†é¢‘ç¤¾åŒº', 'å¾®åšçƒ­æœ', 'ç¤¾äº¤çƒ­æœ']:
            continue
        if not any(k in text_join for k in _ANALYST_HINT_WORDS):
            continue
        likes = int(item.get('likes') or 0)
        if likes < 1000:
            continue
        views.append({
            'target': '',
            'text': text_join,
            'likes': likes,
            'source': item.get('platform', 'èˆ†æƒ…æº'),
        })

    # å»é‡ + æŒ‰çƒ­åº¦æ’åº
    dedup = {}
    for v in views:
        key = _clean_text(v.get('text', ''), 120)
        if not key:
            continue
        old = dedup.get(key)
        if not old or int(v.get('likes', 0)) > int(old.get('likes', 0)):
            dedup[key] = v
    out = sorted(dedup.values(), key=lambda x: int(x.get('likes', 0)), reverse=True)
    return out[:max_items]


def _analyst_snippet(views, keywords, limit=2):
    if not views:
        return ''
    matched = []
    for v in views:
        txt = (v.get('text') or '').lower()
        if any(k.lower() in txt for k in keywords):
            matched.append(v)
    if not matched:
        return ''
    top = matched[:limit]
    pieces = [f"{_clean_text(v.get('text', ''), 60)}ï¼ˆ{v.get('source', 'èˆ†æƒ…æº')}ï¼‰" for v in top]
    return 'ï¼›'.join(pieces)


def _ssl_ctx():
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    return ctx


def fetch_http(url, timeout=15):
    """GET request with timeout and error handling"""
    req = Request(url, headers={
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) fund-assistant/2.0',
        'Accept': 'application/json, text/xml, */*',
    })
    try:
        with urlopen(req, timeout=timeout, context=_ssl_ctx()) as resp:
            return resp.read().decode('utf-8', errors='replace')
    except Exception as e:
        print(f"  [WARN] fetch failed: {url[:80]}... - {e}", file=sys.stderr)
        return None


# ==================== æ–°é—»æºæŠ“å– ====================

def fetch_sina_news():
    """æ–°æµªè´¢ç»æ»šåŠ¨å¿«è®¯ (é«˜ä¿¡å™ªæ¯”, ä¸­æ–‡)"""
    items = []
    raw = fetch_http('https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&k=&num=40&page=1')
    if not raw:
        return items
    try:
        data = json.loads(raw)
        for item in (data.get('result', {}).get('data', []) or []):
            title = (item.get('title') or '').strip()
            if title and len(title) > 8:
                items.append({'title': title, 'source': 'æ–°æµªè´¢ç»', 'time': item.get('ctime', '')})
    except Exception as e:
        print(f"  [WARN] sina parse: {e}", file=sys.stderr)
    return items


def fetch_eastmoney_news():
    """ä¸œæ–¹è´¢å¯Œ7Ã—24å¿«è®¯ (é«˜ä¿¡å™ªæ¯”, ä¸­æ–‡)"""
    items = []
    raw = fetch_http('https://np-listapi.eastmoney.com/comm/web/getNewsByColumns?type=0&client=web&maxNewsId=0&pageSize=40&column=102')
    if not raw:
        return items
    try:
        data = json.loads(raw)
        for item in (data.get('data', {}).get('list', []) or []):
            title = (item.get('title') or '').strip()
            content = (item.get('content') or '').strip()
            text = title or content[:100]
            if text and len(text) > 6:
                items.append({'title': text, 'source': 'ä¸œæ–¹è´¢å¯Œ', 'time': item.get('showTime', '')})
    except Exception as e:
        print(f"  [WARN] eastmoney parse: {e}", file=sys.stderr)
    return items


def fetch_cls_news():
    """è´¢è”ç¤¾å¿«è®¯ (ç”µæŠ¥, æœºæ„çº§ä¿¡å™ªæ¯”)"""
    items = []
    raw = fetch_http('https://www.cls.cn/nodeapi/updateTelegraphList?app=CailianpressWeb&os=web&sv=8.4.6&rn=30')
    if not raw:
        return items
    try:
        data = json.loads(raw)
        for item in (data.get('data', {}).get('roll_data', []) or []):
            title = (item.get('title') or item.get('content', '')[:100]).strip()
            title = re.sub(r'<[^>]+>', '', title)  # strip HTML
            if title and len(title) > 6:
                ctime = item.get('ctime', 0)
                time_str = datetime.fromtimestamp(ctime, tz=timezone(timedelta(hours=8))).isoformat() if ctime else ''
                items.append({'title': title, 'source': 'è´¢è”ç¤¾', 'time': time_str})
    except Exception as e:
        print(f"  [WARN] cls parse: {e}", file=sys.stderr)
    return items


def fetch_rss_bbc():
    """BBC Business RSS (å›½é™…è§†è§’)"""
    import xml.etree.ElementTree as ET
    items = []
    raw = fetch_http('https://feeds.bbci.co.uk/news/business/rss.xml')
    if not raw:
        return items
    try:
        root = ET.fromstring(raw)
        for item in root.findall('.//item')[:15]:
            title = (item.findtext('title') or '').strip()
            if title:
                items.append({'title': title, 'source': 'BBC', 'time': item.findtext('pubDate', '')})
    except Exception as e:
        print(f"  [WARN] BBC RSS: {e}", file=sys.stderr)
    return items


def fetch_rss_reuters():
    """Reuters World/Business News via Google News RSS (åœ°ç¼˜æ”¿æ²»æ ¸å¿ƒæº)"""
    import xml.etree.ElementTree as ET
    items = []
    # Reutersè‡ªæœ‰RSSå·²å…³é—­ï¼Œæ”¹ç”¨Google Newsæœç´¢Reutersæ¥æº
    rss_urls = [
        'https://news.google.com/rss/search?q=site:reuters.com+when:1d&hl=en&gl=US&ceid=US:en',
        'https://news.google.com/rss/search?q=geopolitics+OR+sanctions+OR+OPEC+OR+Iran+OR+tariff+when:1d&hl=en&gl=US&ceid=US:en',
    ]
    for url in rss_urls:
        raw = fetch_http(url)
        if not raw:
            continue
        try:
            root = ET.fromstring(raw)
            for item in root.findall('.//item')[:15]:
                title = (item.findtext('title') or '').strip()
                if title:
                    items.append({'title': title, 'source': 'Reuters/Google', 'time': item.findtext('pubDate', '')})
        except Exception as e:
            print(f"  [WARN] Reuters/Google RSS: {e}", file=sys.stderr)
    # å»é‡
    seen = set()
    unique = []
    for it in items:
        if it['title'] not in seen:
            seen.add(it['title'])
            unique.append(it)
    return unique[:20]


def fetch_rss_aljazeera():
    """Al Jazeera RSS (ä¸­ä¸œ/éæ´²/åœ°ç¼˜è§†è§’)"""
    import xml.etree.ElementTree as ET
    items = []
    raw = fetch_http('https://www.aljazeera.com/xml/rss/all.xml')
    if not raw:
        return items
    try:
        root = ET.fromstring(raw)
        for item in root.findall('.//item')[:12]:
            title = (item.findtext('title') or '').strip()
            if title:
                items.append({'title': title, 'source': 'AlJazeera', 'time': item.findtext('pubDate', '')})
    except Exception as e:
        print(f"  [WARN] AlJazeera RSS: {e}", file=sys.stderr)
    return items


def fetch_rss_ft():
    """Financial Times RSS (å›½é™…è´¢ç»+åœ°ç¼˜)"""
    import xml.etree.ElementTree as ET
    items = []
    rss_urls = [
        'https://www.ft.com/rss/home',
        'https://www.ft.com/world?format=rss',
    ]
    for url in rss_urls:
        raw = fetch_http(url)
        if not raw:
            continue
        try:
            root = ET.fromstring(raw)
            for item in root.findall('.//item')[:10]:
                title = (item.findtext('title') or '').strip()
                if title:
                    items.append({'title': title, 'source': 'FT', 'time': item.findtext('pubDate', '')})
            if items:
                break
        except Exception as e:
            print(f"  [WARN] FT RSS: {e}", file=sys.stderr)
    return items


def fetch_guancha_news():
    """è§‚å¯Ÿè€…ç½‘å›½é™…æ–°é—» (ä¸­æ–‡åœ°ç¼˜è§†è§’)"""
    items = []
    raw = fetch_http('https://www.guancha.cn/internationalNews')
    if not raw:
        return items
    try:
        # ç®€å•æå–æ ‡é¢˜ (HTMLè§£æ)
        titles = re.findall(r'<h4[^>]*>\s*<a[^>]*>([^<]+)</a>\s*</h4>', raw)
        for title in titles[:15]:
            title = title.strip()
            if title and len(title) > 6:
                items.append({'title': title, 'source': 'è§‚å¯Ÿè€…ç½‘', 'time': ''})
    except Exception as e:
        print(f"  [WARN] è§‚å¯Ÿè€…ç½‘: {e}", file=sys.stderr)
    return items


# ==================== é›ªçƒå®æ—¶çƒ­è¯æŠ“å– ====================

# è‚¡ç¥¨å â†’ æ¿å—æ˜ å°„ (ç”¨äºæŠŠé›ªçƒçƒ­è‚¡æ˜ å°„åˆ°åŸºé‡‘æ¿å—ä½“ç³»)
_XQ_STOCK_SECTOR_MAP = {
    # å…³é”®è¯ â†’ æ¿å—åˆ—è¡¨ (æŒ‰è‚¡ç¥¨å/è¡Œä¸šåæ¨¡ç³ŠåŒ¹é…)
    'å®å¾·': ['é”‚ç”µ', 'æ–°èƒ½æº'], 'æ¯”äºšè¿ª': ['æ–°èƒ½æºè½¦', 'æ–°èƒ½æº'], 'ç‰¹æ–¯æ‹‰': ['æ–°èƒ½æºè½¦', 'æ–°èƒ½æº'],
    'éš†åŸº': ['å…‰ä¼', 'æ–°èƒ½æº'], 'é€šå¨': ['å…‰ä¼', 'æ–°èƒ½æº'], 'é˜³å…‰ç”µæº': ['å…‰ä¼', 'æ–°èƒ½æº'],
    'è´µå·èŒ…å°': ['ç™½é…’', 'æ¶ˆè´¹'], 'äº”ç²®æ¶²': ['ç™½é…’', 'æ¶ˆè´¹'], 'æ³¸å·è€çª–': ['ç™½é…’', 'æ¶ˆè´¹'], 'èŒ…å°': ['ç™½é…’', 'æ¶ˆè´¹'],
    'ä¸­èŠ¯': ['åŠå¯¼ä½“', 'AI'], 'éŸ¦å°”': ['åŠå¯¼ä½“', 'AI'], 'åŒ—æ–¹ååˆ›': ['åŠå¯¼ä½“', 'AI'], 'æµ·å…‰': ['åŠå¯¼ä½“', 'AI'],
    'ç´«å…‰': ['åŠå¯¼ä½“', 'AI'], 'ä¸­å¾®': ['åŠå¯¼ä½“', 'AI'], 'å¯’æ­¦çºª': ['AI', 'åŠå¯¼ä½“'],
    'è…¾è®¯': ['æ¸¯è‚¡ç§‘æŠ€', 'AI'], 'é˜¿é‡Œ': ['æ¸¯è‚¡ç§‘æŠ€', 'AI'], 'ç¾å›¢': ['æ¸¯è‚¡ç§‘æŠ€', 'æ¶ˆè´¹'], 'å°ç±³': ['æ¸¯è‚¡ç§‘æŠ€', 'æ¶ˆè´¹'],
    'å­—èŠ‚': ['AI', 'æ¸¯è‚¡ç§‘æŠ€'], 'ç™¾åº¦': ['AI', 'æ¸¯è‚¡ç§‘æŠ€'],
    'è¯æ˜': ['åŒ»è¯', 'åˆ›æ–°è¯'], 'æ’ç‘': ['åŒ»è¯', 'åˆ›æ–°è¯'], 'è¿ˆç‘': ['åŒ»è¯', 'åŒ»ç–—å™¨æ¢°'],
    'ä¸­å›½ä¸­å…': ['æ¶ˆè´¹'], 'æµ·å¤©': ['æ¶ˆè´¹', 'é£Ÿå“é¥®æ–™'],
    'ç´«é‡‘çŸ¿ä¸š': ['æœ‰è‰²é‡‘å±', 'é»„é‡‘'], 'å±±ä¸œé»„é‡‘': ['é»„é‡‘', 'æœ‰è‰²é‡‘å±'], 'ä¸­é‡‘é»„é‡‘': ['é»„é‡‘'],
    'æ´›é˜³é’¼ä¸š': ['æœ‰è‰²é‡‘å±'], 'æ±Ÿè¥¿é“œä¸š': ['æœ‰è‰²é‡‘å±'],
    'ä¸­å›½çŸ³æ²¹': ['èƒ½æº', 'åŸæ²¹'], 'ä¸­å›½çŸ³åŒ–': ['èƒ½æº', 'åŸæ²¹'], 'ä¸­å›½æµ·æ²¹': ['èƒ½æº', 'åŸæ²¹'],
    'ä¸­å›½ç¥å': ['èƒ½æº', 'ç…¤ç‚­'],
    'ä¸­èˆª': ['å†›å·¥'], 'èˆªå¤©': ['å†›å·¥'], 'åŒ—æ–¹å¯¼èˆª': ['å†›å·¥'],
    'æ‹›å•†é“¶è¡Œ': ['çº¢åˆ©', 'é‡‘è'], 'å·¥å•†é“¶è¡Œ': ['çº¢åˆ©', 'é‡‘è'], 'å»ºè®¾é“¶è¡Œ': ['çº¢åˆ©', 'é‡‘è'],
    'é•¿æ±Ÿç”µåŠ›': ['çº¢åˆ©', 'ç”µåŠ›'], 'ä¸­å›½ç§»åŠ¨': ['çº¢åˆ©', 'é€šä¿¡'],
    'ç§‘å¤§è®¯é£': ['AI', 'ç§‘æŠ€'], 'æµªæ½®': ['AI', 'ç®—åŠ›'], 'ä¸­é™…æ—­åˆ›': ['AI', 'ç®—åŠ›'], 'å…‰æ¨¡å—': ['AI', 'ç®—åŠ›'],
    'æœºå™¨äºº': ['AI', 'æœºå™¨äºº'], 'æ±‡å·': ['æœºå™¨äºº', 'åˆ¶é€ '], 'ç»¿çš„è°æ³¢': ['æœºå™¨äºº'],
    'è‹±ä¼Ÿè¾¾': ['AI', 'åŠå¯¼ä½“', 'ç®—åŠ›'], 'NVIDIA': ['AI', 'åŠå¯¼ä½“', 'ç®—åŠ›'],
    'DeepSeek': ['AI', 'å¤§æ¨¡å‹'], 'GPT': ['AI', 'å¤§æ¨¡å‹'], 'AI': ['AI', 'ç§‘æŠ€'],
}

# è¯é¢˜å…³é”®è¯ â†’ æ¿å—æ˜ å°„
_XQ_TOPIC_SECTOR_MAP = {
    'äººå·¥æ™ºèƒ½': ['AI', 'ç§‘æŠ€'], 'èŠ¯ç‰‡': ['åŠå¯¼ä½“', 'AI'], 'åŠå¯¼ä½“': ['åŠå¯¼ä½“'], 'ç®—åŠ›': ['AI', 'ç®—åŠ›'],
    'å¤§æ¨¡å‹': ['AI', 'å¤§æ¨¡å‹'], 'æœºå™¨äºº': ['AI', 'æœºå™¨äºº'], 'æ™ºèƒ½é©¾é©¶': ['æ–°èƒ½æºè½¦', 'AI'],
    'å…‰ä¼': ['å…‰ä¼', 'æ–°èƒ½æº'], 'æ–°èƒ½æº': ['æ–°èƒ½æº'], 'é”‚ç”µ': ['é”‚ç”µ', 'æ–°èƒ½æº'], 'å‚¨èƒ½': ['æ–°èƒ½æº', 'å‚¨èƒ½'],
    'ç™½é…’': ['ç™½é…’', 'æ¶ˆè´¹'], 'æ¶ˆè´¹': ['æ¶ˆè´¹'], 'é£Ÿå“': ['æ¶ˆè´¹', 'é£Ÿå“é¥®æ–™'],
    'åŒ»è¯': ['åŒ»è¯'], 'åˆ›æ–°è¯': ['åŒ»è¯', 'åˆ›æ–°è¯'], 'ä¸­è¯': ['åŒ»è¯'],
    'å†›å·¥': ['å†›å·¥'], 'å›½é˜²': ['å†›å·¥'], 'èˆªå¤©': ['å†›å·¥'],
    'é»„é‡‘': ['é»„é‡‘'], 'é“œ': ['æœ‰è‰²é‡‘å±'], 'æœ‰è‰²': ['æœ‰è‰²é‡‘å±'], 'ç¨€åœŸ': ['æœ‰è‰²é‡‘å±'],
    'åŸæ²¹': ['èƒ½æº', 'åŸæ²¹'], 'çŸ³æ²¹': ['èƒ½æº', 'åŸæ²¹'], 'ç…¤ç‚­': ['èƒ½æº'],
    'æ¸¯è‚¡': ['æ¸¯è‚¡ç§‘æŠ€'], 'æ’ç”Ÿ': ['æ¸¯è‚¡ç§‘æŠ€'], 'ç§‘æŠ€': ['ç§‘æŠ€', 'AI'],
    'é“¶è¡Œ': ['çº¢åˆ©', 'é‡‘è'], 'çº¢åˆ©': ['çº¢åˆ©'], 'é«˜è‚¡æ¯': ['çº¢åˆ©'],
    'å€ºåˆ¸': ['å€ºåˆ¸'], 'åˆ©ç‡': ['å€ºåˆ¸', 'é‡‘è'],
    'åœ°äº§': ['åœ°äº§'], 'æˆ¿åœ°äº§': ['åœ°äº§'], 'åŸºå»º': ['åŸºå»º'],
    'å…‰æ¨¡å—': ['AI', 'ç®—åŠ›'], 'æ•°æ®ä¸­å¿ƒ': ['AI', 'ç®—åŠ›'], 'äº‘è®¡ç®—': ['AI', 'ç§‘æŠ€'],
    'ETF': ['å®½åŸº'], 'æ²ªæ·±300': ['å®½åŸº'], 'ä¸­è¯500': ['å®½åŸº'],
}


def _get_xueqiu_opener():
    """åˆ›å»ºå¸¦cookieçš„è¯·æ±‚å™¨(é›ªçƒAPIéœ€è¦å…ˆè·å–cookie)"""
    cj = CookieJar()
    opener = build_opener(HTTPCookieProcessor(cj), HTTPSHandler(context=_ssl_ctx()))
    opener.addheaders = [
        ('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'),
        ('Accept', 'application/json, text/plain, */*'),
        ('Origin', 'https://xueqiu.com'),
        ('Referer', 'https://xueqiu.com/'),
    ]
    if XUEQIU_COOKIE:
        opener.addheaders.append(('Cookie', XUEQIU_COOKIE))
    # å…ˆè®¿é—®ä¸»é¡µè·å–cookie
    try:
        resp = opener.open(Request('https://xueqiu.com/', headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }), timeout=10)
        resp.read()
    except Exception as e:
        print(f"  [WARN] é›ªçƒcookieè·å–å¤±è´¥: {e}", file=sys.stderr)
    return opener


def fetch_xueqiu_hot_stocks(opener=None):
    """é›ªçƒçƒ­è‚¡æ¦œ â†’ [{name, code, percent, heat, current}]"""
    if not opener:
        opener = _get_xueqiu_opener()
    items = []
    # çƒ­åº¦æ’è¡Œ (å…³æ³¨åº¦æ’è¡Œ)
    urls = [
        'https://stock.xueqiu.com/v5/stock/hot_stock/list.json?size=30&_type=10&type=10',
        'https://stock.xueqiu.com/v5/stock/hot_stock/list.json?size=30&_type=12&type=12',
    ]
    for url in urls:
        try:
            resp = opener.open(Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': 'https://xueqiu.com/',
            }), timeout=12)
            body = resp.read().decode('utf-8', errors='replace')
            data = json.loads(body)
            for stock in (data.get('data', {}).get('items', []) or []):
                name = stock.get('name', '')
                code = stock.get('code', stock.get('symbol', ''))
                percent = stock.get('percent', stock.get('current_year_percent', 0)) or 0
                value = stock.get('value', stock.get('current', 0)) or 0
                if name:
                    items.append({
                        'name': name,
                        'code': str(code),
                        'percent': round(float(percent), 2) if percent else 0,
                        'heat': int(value) if value else 0,
                    })
        except Exception as e:
            print(f"  [WARN] é›ªçƒçƒ­è‚¡: {e}", file=sys.stderr)
    # å»é‡
    seen = set()
    unique = []
    for it in items:
        if it['name'] not in seen:
            seen.add(it['name'])
            unique.append(it)
    return unique[:30]


def fetch_xueqiu_hot_topics(opener=None):
    """é›ªçƒçƒ­å¸–/çƒ­è®®è¯é¢˜ â†’ [{text, retweet_count, reply_count, like_count}]"""
    if not opener:
        opener = _get_xueqiu_opener()
    items = []
    urls = [
        'https://xueqiu.com/statuses/hot/listV2.json?since_id=-1&max_id=-1&size=20',
    ]
    for url in urls:
        try:
            resp = opener.open(Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': 'https://xueqiu.com/',
            }), timeout=12)
            body = resp.read().decode('utf-8', errors='replace')
            data = json.loads(body)
            for item in (data.get('items', []) or []):
                og = item.get('original_status', item)
                text = (og.get('text') or og.get('title') or og.get('description') or '')
                text = re.sub(r'<[^>]+>', '', text).strip()[:200]
                if not text or len(text) < 6:
                    continue
                items.append({
                    'text': text,
                    'retweet_count': og.get('retweet_count', 0) or 0,
                    'reply_count': og.get('reply_count', 0) or 0,
                    'like_count': og.get('like_count', 0) or 0,
                })
        except Exception as e:
            print(f"  [WARN] é›ªçƒçƒ­å¸–: {e}", file=sys.stderr)
    return items[:20]


def _map_stock_to_sectors(name):
    """å°†è‚¡ç¥¨åæ˜ å°„åˆ°æ¿å—åˆ—è¡¨"""
    sectors = set()
    for kw, sector_list in _XQ_STOCK_SECTOR_MAP.items():
        if kw in name:
            sectors.update(sector_list)
    return list(sectors) if sectors else ['å…¶ä»–']


def _extract_topic_sectors(text):
    """ä»è¯é¢˜æ–‡æœ¬æå–ç›¸å…³æ¿å—"""
    sectors = set()
    for kw, sector_list in _XQ_TOPIC_SECTOR_MAP.items():
        if kw in text:
            sectors.update(sector_list)
    # ä¹Ÿå°è¯•ç”¨è‚¡ç¥¨åæ˜ å°„
    for kw, sector_list in _XQ_STOCK_SECTOR_MAP.items():
        if kw in text:
            sectors.update(sector_list)
    return list(sectors)


def process_xueqiu_to_hotwords(hot_stocks, hot_topics):
    """
    å°†é›ªçƒçƒ­è‚¡+çƒ­å¸–æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†çƒ­è¯æ ¼å¼:
    [{word, heat, trend, sources:['é›ªçƒ'], relatedSectors:[...]}]
    """
    hotwords = []

    # 1. çƒ­è‚¡ â†’ çƒ­è¯
    if hot_stocks:
        max_heat = max(s.get('heat', 1) for s in hot_stocks) or 1
        for s in hot_stocks:
            name = s['name']
            sectors = _map_stock_to_sectors(name)
            if 'å…¶ä»–' in sectors and len(sectors) == 1:
                continue  # è·³è¿‡æ— æ³•æ˜ å°„çš„
            raw_heat = s.get('heat', 0)
            normalized_heat = int(5000 + (raw_heat / max_heat) * 5000) if max_heat > 0 else 5000
            trend = 'up' if s.get('percent', 0) > 1 else ('down' if s.get('percent', 0) < -1 else 'stable')
            hotwords.append({
                'word': name,
                'heat': normalized_heat,
                'trend': trend,
                'sources': ['é›ªçƒ'],
                'relatedSectors': sectors,
                'type': 'stock',
                'percent': s.get('percent', 0),
            })

    # 2. çƒ­å¸– â†’ è¯é¢˜çƒ­è¯ (æå–å…³é”®è¯)
    topic_sector_count = {}  # æ¿å— â†’ å‡ºç°æ¬¡æ•° + çƒ­åº¦
    if hot_topics:
        for t in hot_topics:
            text = t.get('text', '')
            sectors = _extract_topic_sectors(text)
            engagement = (t.get('retweet_count', 0) + t.get('reply_count', 0) + t.get('like_count', 0))
            for s in sectors:
                if s not in topic_sector_count:
                    topic_sector_count[s] = {'count': 0, 'engagement': 0, 'texts': []}
                topic_sector_count[s]['count'] += 1
                topic_sector_count[s]['engagement'] += engagement
                if len(topic_sector_count[s]['texts']) < 2:
                    # æˆªå–æ ‡é¢˜çº§æ‘˜è¦
                    short = text[:30].replace('\n', ' ')
                    topic_sector_count[s]['texts'].append(short)

    # æŠŠè¯é¢˜èšåˆä¸ºæ¿å—çƒ­è¯
    for sector, info in topic_sector_count.items():
        if info['count'] < 1:
            continue
        heat = min(10000, 3000 + info['count'] * 1500 + info['engagement'] // 100)
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰åŒæ¿å—çš„çƒ­è‚¡è¯
        exists = any(sector in hw['relatedSectors'] for hw in hotwords)
        if exists:
            # è¿½åŠ åˆ°å·²æœ‰åŒæ¿å—çƒ­è¯çš„çƒ­åº¦
            for hw in hotwords:
                if sector in hw['relatedSectors']:
                    hw['heat'] = min(10000, hw['heat'] + info['count'] * 300)
                    break
        else:
            hotwords.append({
                'word': f"{sector}(é›ªçƒçƒ­è®®)",
                'heat': heat,
                'trend': 'up' if info['count'] >= 3 else 'stable',
                'sources': ['é›ªçƒ'],
                'relatedSectors': [sector],
                'type': 'topic',
            })

    # æ’åº & å»é‡
    hotwords.sort(key=lambda x: x.get('heat', 0), reverse=True)
    return hotwords[:25]


def fetch_xueqiu_hotwords():
    """å®Œæ•´é›ªçƒçƒ­è¯æŠ“å–æµç¨‹: cookie â†’ çƒ­è‚¡ + çƒ­å¸– â†’ æ ‡å‡†çƒ­è¯æ ¼å¼"""
    print("  ğŸ”¥ é›ªçƒçƒ­è¯æŠ“å–...")
    try:
        opener = _get_xueqiu_opener()
        stocks = fetch_xueqiu_hot_stocks(opener)
        print(f"    çƒ­è‚¡: {len(stocks)} æ¡")
        topics = fetch_xueqiu_hot_topics(opener)
        print(f"    çƒ­å¸–: {len(topics)} æ¡")
        hotwords = process_xueqiu_to_hotwords(stocks, topics)
        print(f"    çƒ­è¯: {len(hotwords)} æ¡")
        return {
            'hotwords': hotwords,
            'hot_stocks': stocks[:15],  # ä¿ç•™åŸå§‹æ•°æ®ä¾›å‰ç«¯ç›´æ¥å±•ç¤º
            'hot_topics': [{'text': t['text'][:100], 'engagement': t.get('retweet_count',0)+t.get('reply_count',0)+t.get('like_count',0)} for t in topics[:10]],
            'fetched_at': datetime.now(timezone(timedelta(hours=8))).isoformat(),
        }
    except Exception as e:
        print(f"  âŒ é›ªçƒçƒ­è¯æŠ“å–å¤±è´¥: {e}", file=sys.stderr)
        return None


def is_valid_xueqiu_data(data):
    """é›ªçƒæ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆè‡³å°‘æœ‰çƒ­è¯æˆ–çƒ­è‚¡ï¼‰"""
    if not isinstance(data, dict):
        return False
    hotwords = data.get('hotwords') or []
    hot_stocks = data.get('hot_stocks') or []
    return len(hotwords) > 0 or len(hot_stocks) > 0


# ==================== LLM ç»“æ„åŒ–æå– ====================

def call_llm(news_items):
    """è°ƒç”¨å¤§æ¨¡å‹å°†æ–°é—»åˆ—è¡¨ â†’ ç»“æ„åŒ–äº‹ä»¶ + çƒ­åº¦æ ‡ç­¾"""
    if not API_KEY:
        print("[ERROR] AI_API_KEY not set, cannot call LLM", file=sys.stderr)
        return None

    # æ„å»ºæ–°é—»æ–‡æœ¬ (å»é‡, é™åˆ¶é•¿åº¦)
    seen = set()
    unique = []
    for n in news_items:
        key = re.sub(r'\W', '', n['title'])[:30]
        if key not in seen:
            seen.add(key)
            unique.append(n)
    news_text = '\n'.join([
        f"{i+1}. [{n['source']}] {n['title']}"
        for i, n in enumerate(unique[:50])
    ])

    tags_str = 'ã€'.join(MARKET_TAGS)

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªé¡¶çº§é‡åŒ–é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿ä»è´¢ç»æ–°é—»ä¸­æå–æŠ•èµ„ä¿¡å·ã€‚

## ä»»åŠ¡ä¸€ï¼šæå–é«˜å½±å“åŠ›æ”¿ç»äº‹ä»¶
ä»æ–°é—»ä¸­**ä»…æå–æ”¿æ²»ã€ç»æµã€é‡‘èæ”¿ç­–ã€åœ°ç¼˜æ”¿æ²»ã€å¤®è¡Œè´§å¸æ”¿ç­–ã€è´¢æ”¿æ”¿ç­–ã€è´¸æ˜“æ”¿ç­–ã€äº§ä¸šæ”¿ç­–ã€å¤§å®—å•†å“ä¾›éœ€å˜åŒ–**ç­‰æ”¿ç»ç±»äº‹ä»¶ã€‚
ä¸¥æ ¼è¿‡æ»¤æ‰ä»¥ä¸‹æ— å…³å†…å®¹ï¼šå¨±ä¹å…«å¦ã€ä½“è‚²èµ›äº‹ã€ç¤¾ä¼šæ–°é—»ã€å¤©æ°”ã€ç”Ÿæ´»æ–¹å¼ã€æ˜æ˜Ÿã€ç»¼è‰ºç­‰ã€‚
æå–å½±å“åŠ›â‰¥3æ˜Ÿ(æ»¡5æ˜Ÿ)çš„é‡å¤§æ”¿ç»äº‹ä»¶ï¼Œåˆå¹¶åŒç±»æ–°é—»ã€‚
æ¯ä¸ªäº‹ä»¶å¿…é¡»æ ‡æ³¨å½±å“çš„è¡Œä¸šæ¿å—(æ­£é¢/è´Ÿé¢)ã€‚æœ€å¤šè¾“å‡º12æ¡äº‹ä»¶ã€‚

**é‡è¦ï¼šç¡®ä¿äº‹ä»¶è¦†ç›–å°½å¯èƒ½å¤šçš„è¡Œä¸šæ¿å—ã€‚** é™¤äº†AI/ç§‘æŠ€ã€å€ºåˆ¸ç­‰çƒ­é—¨æ¿å—ä»¥å¤–ï¼Œå¿…é¡»ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹æ¿å—çš„ç›¸å…³æ–°é—»å¹¶æå–äº‹ä»¶ï¼š
- **åœ°ç¼˜æ”¿æ²»(æœ€é«˜ä¼˜å…ˆçº§!)**: ç¾ä¼Šå…³ç³»ã€ä¿„ä¹Œå†²çªã€ä¸­ç¾åšå¼ˆã€ä¸­ä¸œå±€åŠ¿ã€çº¢æµ·èˆªè¿ã€éæ´²èµ„æºå›½æ”¿ç­–(é”‚çŸ¿/é’´çŸ¿å‡ºå£ç¦ä»¤)ã€OPECå‡äº§ã€å…³ç¨æˆ˜ã€åˆ¶è£æªæ–½ç­‰ã€‚åœ°ç¼˜äº‹ä»¶å¯¹æ²¹æ°”ã€æœ‰è‰²é‡‘å±ã€é»„é‡‘ã€å†›å·¥æ¿å—å½±å“æå¤§ï¼Œå¿…é¡»æå–ï¼
- **å¤§å®—å•†å“**: åŸæ²¹/æ²¹æ°”ä»·æ ¼å˜åŠ¨ã€OPECå†³ç­–ã€é“œé“ç­‰æœ‰è‰²é‡‘å±æ¶¨è·Œã€é»„é‡‘ç™½é“¶èµ°åŠ¿ã€é”‚çŸ¿/ç¨€åœŸä¾›åº”é“¾
- **èƒ½æºä¸èµ„æº**: èƒ½æºæ”¿ç­–ã€çŸ¿äº§èµ„æºä¾›éœ€ã€ç¢³æ’æ”¾æ”¿ç­–ã€éæ´²/å—ç¾èµ„æºå›½å‡ºå£é™åˆ¶
- **æ¶ˆè´¹ä¸å†…éœ€**: ç¤¾é›¶æ•°æ®ã€æ¶ˆè´¹æ”¿ç­–ã€ç™½é…’/é£Ÿå“è¡Œä¸šåŠ¨æ€
- **åŒ»è¯å¥åº·**: åŒ»è¯æ”¿ç­–ã€é›†é‡‡ã€åˆ›æ–°è¯å®¡æ‰¹
- **å†›å·¥å›½é˜²**: å†›è´¹é¢„ç®—ã€è£…å¤‡é‡‡è´­ã€åœ°ç¼˜å†²çªé©±åŠ¨çš„å†›å·¥éœ€æ±‚
å¦‚æœæ–°é—»ä¸­æœ‰æ¶‰åŠæœ‰è‰²é‡‘å±(é“œã€é“ã€é”Œã€ç¨€åœŸã€é”‚ç­‰)ã€åŸæ²¹/æ²¹æ°”ã€é»„é‡‘ç™½é“¶ç­‰å¤§å®—å•†å“çš„å†…å®¹ï¼ŒåŠ¡å¿…å•ç‹¬æå–ä¸ºäº‹ä»¶ã€‚
å¦‚æœæ–°é—»ä¸­æœ‰æ¶‰åŠå›½é™…åœ°ç¼˜å†²çª(ç¾ä¼Šã€ä¿„ä¹Œã€ä¸­ç¾ã€çº¢æµ·ç­‰)çš„å†…å®¹ï¼ŒåŠ¡å¿…å•ç‹¬æå–ä¸ºäº‹ä»¶å¹¶æ ‡æ³¨å½±å“çš„æ¿å—(å¦‚æ²¹æ°”ã€å†›å·¥ã€é»„é‡‘ç­‰)ã€‚

sectors_positive å’Œ sectors_negative å­—æ®µåº”ä½¿ç”¨ä»¥ä¸‹æ ‡å‡†æ¿å—åï¼š
AI/ç§‘æŠ€ã€åŠå¯¼ä½“ã€ç®—åŠ›ã€AIGCã€æ–°èƒ½æºã€å…‰ä¼ã€é”‚ç”µã€æ–°èƒ½æºè½¦ã€æ¶ˆè´¹ã€é£Ÿå“é¥®æ–™ã€ç™½é…’ã€åŒ»è¯ã€åˆ›æ–°è¯ã€
é»„é‡‘ã€è´µé‡‘å±ã€æœ‰è‰²é‡‘å±ã€é“œé“ã€å¤§å®—å•†å“ã€èƒ½æºã€åŸæ²¹ã€æ²¹æ°”ã€
å†›å·¥ã€å›½é˜²ã€çº¢åˆ©ã€é«˜è‚¡æ¯ã€å€ºåˆ¸ã€å›ºæ”¶ã€é‡‘èã€é“¶è¡Œã€åˆ¸å•†ã€
æ¸¯è‚¡ç§‘æŠ€ã€æ¸¯è‚¡äº’è”ç½‘ã€æ’ç”Ÿç§‘æŠ€ã€åœ°äº§ã€åŸºå»ºã€å®½åŸº

fund_keywords å­—æ®µåº”åŒ…å«èƒ½åŒ¹é…åˆ°åŸºé‡‘åç§°/ç±»å‹çš„å…³é”®è¯ï¼Œå¦‚: äººå·¥æ™ºèƒ½ã€AIã€ç®—åŠ›ã€é»„é‡‘ã€æœ‰è‰²é‡‘å±ã€æ²¹æ°”ã€åŸæ²¹ã€æ–°èƒ½æºã€åŠå¯¼ä½“ã€å†›å·¥ã€æ¶ˆè´¹ã€åŒ»è¯ã€çº¢åˆ©ç­‰ã€‚

## ä»»åŠ¡äºŒï¼šç”Ÿæˆå¸‚åœºæ ‡ç­¾çƒ­åº¦
åŸºäºæ‰€æœ‰æ–°é—»çš„ç»¼åˆè¯­ä¹‰åˆ†æï¼Œä¸ºä»¥ä¸‹21ä¸ªå¸‚åœºæ ‡ç­¾è¯„ä¼°çƒ­åº¦å’Œæƒ…ç»ªï¼š
{tags_str}
- temperature: 0-100ï¼Œåæ˜ å½“å‰å¸‚åœºå…³æ³¨åº¦ (50=æ­£å¸¸, 80+=é«˜çƒ­, 20-=å†°å†·)
- sentiment: -1åˆ°+1ï¼Œåæ˜ åˆ©å¥½/åˆ©ç©ºæ–¹å‘

## ä¸¥æ ¼è¾“å‡ºæ ¼å¼ (çº¯JSONï¼Œä¸è¦markdown/æ³¨é‡Š/å¤šä½™æ–‡å­—)ï¼š
{{
  "events": [
    {{
      "title": "ä¸€å¥è¯äº‹ä»¶æ‘˜è¦(15å­—å†…)",
      "category": "technology|geopolitics|monetary|policy|commodity|market",
      "concepts": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
      "sentiment": 0.8,
      "impact": 4,
      "sectors_positive": ["AI/ç§‘æŠ€", "åŠå¯¼ä½“"],
      "sectors_negative": [],
      "fund_keywords": ["äººå·¥æ™ºèƒ½", "AI", "ç®—åŠ›"],
      "reason": "30å­—å†…ç®€æ",
      "advice": "15å­—å†…æ“ä½œå»ºè®®"
    }}
  ],
  "heatmap": [
    {{ "tag": "äººå·¥æ™ºèƒ½", "temperature": 85, "sentiment": 0.8 }}
  ],
  "outlook_summary": "50å­—å†…å¸‚åœºæ€»è§ˆ"
}}"""

    payload = json.dumps({
        "model": MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹{len(unique)}æ¡æœ€æ–°æ–°é—»ï¼Œåªæå–æ”¿æ²»ç»æµé‡‘èç›¸å…³äº‹ä»¶ï¼Œå¿½ç•¥å¨±ä¹ä½“è‚²ç¤¾ä¼šç­‰æ— å…³æ–°é—»ï¼š\n\n{news_text}"}
        ],
        "temperature": 0.2,
        "max_tokens": 4096,
    })

    req = Request(
        f"{API_BASE}/chat/completions",
        data=payload.encode('utf-8'),
        headers={
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {API_KEY}',
        }
    )

    try:
        with urlopen(req, timeout=90, context=_ssl_ctx()) as resp:
            result = json.loads(resp.read().decode('utf-8'))
            content = result['choices'][0]['message']['content']
            # strip markdown code fences
            content = re.sub(r'```json\s*', '', content)
            content = re.sub(r'```\s*', '', content)
            return json.loads(content.strip())
    except Exception as e:
        print(f"[ERROR] LLM call failed: {e}", file=sys.stderr)
        return None


# ==================== æ•°æ®ç»„è£… ====================

def load_previous():
    """åŠ è½½ä¸Šæ¬¡æ•°æ®, ç”¨äºè®¡ç®—è¶‹åŠ¿"""
    try:
        with open(OUTPUT_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return None


def compute_trend(current_temp, prev_data, tag):
    """å¯¹æ¯”ä¸Šæ¬¡æ¸©åº¦, åˆ¤æ–­è¶‹åŠ¿æ–¹å‘"""
    if not prev_data or 'heatmap' not in prev_data:
        return 'stable'
    prev = next((h for h in prev_data['heatmap'] if h['tag'] == tag), None)
    if not prev:
        return 'new'
    diff = current_temp - prev.get('temperature', 50)
    if diff > 5:
        return 'up'
    if diff < -5:
        return 'down'
    return 'stable'


def enrich_event(evt, idx, now):
    """è¡¥å…¨äº‹ä»¶å­—æ®µ, è®¡ç®—impact/confidence, ç”Ÿæˆid"""
    # ä»conceptsæ¨å¯¼ sectors_positive/negative (å¦‚æœLLMæ²¡è¿”å›)
    if not evt.get('sectors_positive'):
        sectors = []
        for concept in evt.get('concepts', []):
            sectors.extend(TAG_TO_SECTORS.get(concept, []))
        evt['sectors_positive'] = list(dict.fromkeys(sectors))[:5]  # dedup

    if 'sectors_negative' not in evt:
        evt['sectors_negative'] = []

    # è®¡ç®— confidence (sentimentå¼ºåº¦ + impactçº§åˆ«)
    sentiment = evt.get('sentiment', 0)
    impact_level = evt.get('impact', 3)
    confidence = min(1.0, max(0.3, abs(sentiment) * 0.5 + impact_level * 0.12))

    # å°† impact_level (1-5) æ˜ å°„åˆ°å®é™… impact å€¼ (-20 to +20)
    impact_value = int(sentiment * impact_level * 4)
    impact_value = max(-20, min(20, impact_value))

    return {
        "id": f"evt_{now.strftime('%Y%m%d')}_{idx+1:03d}",
        "title": evt.get('title', ''),
        "category": evt.get('category', 'market'),
        "concepts": evt.get('concepts', []),
        "sentiment": round(sentiment, 2),
        "impact": impact_value,
        "confidence": round(confidence, 2),
        "sectors_positive": evt.get('sectors_positive', []),
        "sectors_negative": evt.get('sectors_negative', []),
        "fund_keywords": evt.get('fund_keywords', []),
        "reason": evt.get('reason', ''),
        "advice": evt.get('advice', ''),
        "source": "AIç»¼åˆåˆ†æ",
        "time": now.isoformat(),
    }


# å¤§å®—å•†å“/èµ„æºå¸¸é©»äº‹ä»¶æ¨¡æ¿ (LLMæœªè¦†ç›–æ—¶è‡ªåŠ¨æ³¨å…¥)
_COMMODITY_FALLBACKS = [
    {
        "key_sectors": {'é»„é‡‘', 'è´µé‡‘å±'},
        "template": {
            "title": "é¿é™©èµ„äº§å—é’ç",
            "category": "commodity",
            "concepts": ["é»„é‡‘"],
            "sentiment": 0.3,
            "impact": 3,
            "sectors_positive": ["é»„é‡‘", "è´µé‡‘å±", "æœ‰è‰²é‡‘å±"],
            "sectors_negative": [],
            "fund_keywords": ["é»„é‡‘", "è´µé‡‘å±", "é¿é™©", "ç™½é“¶", "æœ‰è‰²é‡‘å±"],
            "reason": "åœ°ç¼˜ä¸ç¡®å®šæ€§+å¤®è¡Œè´­é‡‘ï¼Œé»„é‡‘ä½œä¸ºé¿é™©èµ„äº§ç»´æŒå…³æ³¨",
            "advice": "é»„é‡‘ETFä½œåº•ä»“é…ç½®",
        },
    },
    {
        "key_sectors": {'æœ‰è‰²é‡‘å±', 'é“œé“', 'å¤§å®—å•†å“'},
        "template": {
            "title": "å…¨çƒæœ‰è‰²é‡‘å±éœ€æ±‚æ—ºç››",
            "category": "commodity",
            "concepts": ["æœ‰è‰²é‡‘å±"],
            "sentiment": 0.3,
            "impact": 3,
            "sectors_positive": ["æœ‰è‰²é‡‘å±", "é“œé“", "å¤§å®—å•†å“", "èµ„æº"],
            "sectors_negative": [],
            "fund_keywords": ["æœ‰è‰²é‡‘å±", "é“œ", "é“", "èµ„æº", "çŸ¿ä¸š"],
            "reason": "æ–°åŸºå»º+æ–°èƒ½æºè½¦å¸¦åŠ¨é“œé“éœ€æ±‚ï¼Œæœ‰è‰²é‡‘å±ç»´æŒç»“æ„æ€§è¡Œæƒ…",
            "advice": "æœ‰è‰²é‡‘å±ETFæ³¢æ®µæ“ä½œ",
        },
    },
    {
        "key_sectors": {'èƒ½æº', 'åŸæ²¹', 'æ²¹æ°”'},
        "template": {
            "title": "å›½é™…æ²¹ä»·æ³¢åŠ¨åŠ å‰§",
            "category": "commodity",
            "concepts": ["åŸæ²¹"],
            "sentiment": 0.1,
            "impact": 2,
            "sectors_positive": ["èƒ½æº", "åŸæ²¹", "æ²¹æ°”", "å¤§å®—å•†å“"],
            "sectors_negative": [],
            "fund_keywords": ["åŸæ²¹", "æ²¹æ°”", "çŸ³æ²¹", "å¤©ç„¶æ°”", "èƒ½æº"],
            "reason": "OPECå‡äº§é¢„æœŸ+åœ°ç¼˜å†²çªï¼Œæ²¹æ°”ä»·æ ¼æ³¢åŠ¨ä¿¡å·",
            "advice": "æ²¹æ°”ETFå…³æ³¨ä¾›ç»™ç«¯å˜åŒ–",
        },
    },
]


def _ensure_commodity_events(events, now):
    """ç¡®ä¿å¤§å®—å•†å“æ ¸å¿ƒæ¿å—å§‹ç»ˆæœ‰äº‹ä»¶è¦†ç›– (LLMå¯èƒ½é—æ¼)"""
    all_sectors = set()
    for e in events:
        all_sectors.update(e.get('sectors_positive', []))
        all_sectors.update(e.get('sectors_negative', []))

    added = 0
    for fb in _COMMODITY_FALLBACKS:
        if not fb['key_sectors'] & all_sectors:
            # è¯¥æ¿å—æœªè¢«ä»»ä½•åŠ¨æ€äº‹ä»¶è¦†ç›– â†’ æ³¨å…¥å¸¸é©»äº‹ä»¶
            idx = len(events)
            evt = dict(fb['template'])
            evt['id'] = f"evt_{now.strftime('%Y%m%d')}_base_{idx+1:03d}"
            evt['confidence'] = 0.6
            evt['source'] = "å¸¸é©»åŸºç¡€äº‹ä»¶"
            evt['time'] = now.isoformat()
            events.append(evt)
            added += 1
            print(f"  ğŸ“Œ è¡¥å……å¸¸é©»äº‹ä»¶: {evt['title']} (åŠ¨æ€äº‹ä»¶æœªè¦†ç›– {fb['key_sectors']})")

    if added:
        print(f"  å…±è¡¥å…… {added} ä¸ªå¤§å®—å•†å“å¸¸é©»äº‹ä»¶")
    return events


# åœ°ç¼˜æ”¿æ²»å¸¸é©»äº‹ä»¶æ¨¡æ¿ (æŒç»­æ€§åœ°ç¼˜é£é™©ï¼Œå³ä½¿LLMæœªå•ç‹¬æå–ä¹Ÿåº”è¿½è¸ª)
_GEOPOLITICAL_FALLBACKS = [
    {
        "key_keywords": ['ä¼Šæœ—', 'iran', 'ä¸­ä¸œ', 'éœå°”æœ¨å…¹', 'çº¢æµ·', 'èƒ¡å¡', 'houthi'],
        "key_sectors": {'èƒ½æº', 'åŸæ²¹', 'æ²¹æ°”'},
        "template": {
            "title": "ä¸­ä¸œåœ°ç¼˜å±€åŠ¿ç´§å¼ ",
            "category": "geopolitics",
            "concepts": ["åŸæ²¹", "é»„é‡‘"],
            "sentiment": -0.3,
            "impact": 3,
            "sectors_positive": ["åŸæ²¹", "èƒ½æº", "æ²¹æ°”", "å¤§å®—å•†å“", "é»„é‡‘", "è´µé‡‘å±"],
            "sectors_negative": ["èˆªç©º", "æ¶ˆè´¹"],
            "fund_keywords": ["åŸæ²¹", "æ²¹æ°”", "çŸ³æ²¹", "å¤©ç„¶æ°”", "èƒ½æº", "é»„é‡‘", "é¿é™©"],
            "reason": "ç¾ä¼Šå…³ç³»ç´§å¼ +çº¢æµ·èˆªè¿å—é˜»ï¼Œæ¨å‡æ²¹ä»·å’Œé¿é™©èµ„äº§",
            "advice": "æ²¹æ°”+é»„é‡‘å¯¹å†²é…ç½®",
        },
    },
    {
        'key_keywords': ['ç¾ä¿„', 'ä¹Œå…‹å…°', 'ä¿„ä¹Œ', 'russia', 'ukraine', 'nato', 'NATO'],
        "key_sectors": {'å†›å·¥', 'å›½é˜²'},
        "template": {
            "title": "ä¿„ä¹Œå†²çªä¸åˆ¶è£å½±å“å»¶ç»­",
            "category": "geopolitics",
            "concepts": ["å†›å·¥", "åŸæ²¹"],
            "sentiment": -0.2,
            "impact": 3,
            "sectors_positive": ["å†›å·¥", "å›½é˜²", "èƒ½æº", "é»„é‡‘", "è´µé‡‘å±"],
            "sectors_negative": ["æ¶ˆè´¹", "èˆªç©º"],
            "fund_keywords": ["å†›å·¥", "å›½é˜²", "èˆªå¤©", "é»„é‡‘", "åŸæ²¹", "èƒ½æº"],
            "reason": "ä¿„ä¹Œå†²çªæŒç»­ï¼Œæ¨å‡å†›å·¥+èƒ½æºéœ€æ±‚ï¼Œé¿é™©æƒ…ç»ªå—ç›Šé»„é‡‘",
            "advice": "å†›å·¥ETF+é»„é‡‘åº•ä»“",
        },
    },
    {
        "key_keywords": ['ç¾ä¸­', 'ä¸­ç¾', 'åˆ¶è£', 'å…³ç¨', 'tariff', 'sanction', 'èŠ¯ç‰‡ç¦ä»¤', 'ç§‘æŠ€æˆ˜'],
        "key_sectors": {'åŠå¯¼ä½“', 'AI/ç§‘æŠ€'},
        "template": {
            "title": "ä¸­ç¾ç§‘æŠ€åšå¼ˆå»¶ç»­",
            "category": "geopolitics",
            "concepts": ["åŠå¯¼ä½“", "AIç®—åŠ›"],
            "sentiment": -0.3,
            "impact": 3,
            "sectors_positive": ["åŠå¯¼ä½“", "å†›å·¥", "AI/ç§‘æŠ€"],
            "sectors_negative": ["æ¶ˆè´¹", "è´¸æ˜“ç›¸å…³"],
            "fund_keywords": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "ç§‘æŠ€", "AI", "å†›å·¥"],
            "reason": "ä¸­ç¾ç§‘æŠ€è„±é’©åŠ é€Ÿï¼ŒåŠå¯¼ä½“å›½äº§æ›¿ä»£+å†›å·¥è‡ªä¸»æ”»å…³å—ç›Š",
            "advice": "åŠå¯¼ä½“+å†›å·¥å›½äº§æ›¿ä»£ä¸»çº¿",
        },
    },
    {
        'key_keywords': ['é”‚çŸ¿', 'ç¨€åœŸ', 'å‡ºå£ç¦', 'çŸ¿äº§', 'éæ´²', 'æ™ºåˆ©', 'åˆšæœ', 'lithium', 'rare earth', 'cobalt'],
        "key_sectors": {'æœ‰è‰²é‡‘å±', 'æ–°èƒ½æº'},
        "template": {
            "title": "å…¨çƒå…³é”®çŸ¿äº§ä¾›åº”é“¾ç´§å¼ ",
            "category": "geopolitics",
            "concepts": ["æœ‰è‰²é‡‘å±", "é”‚ç”µ"],
            "sentiment": 0.3,
            "impact": 3,
            "sectors_positive": ["æœ‰è‰²é‡‘å±", "é“œé“", "å¤§å®—å•†å“", "é”‚ç”µ", "æ–°èƒ½æº"],
            "sectors_negative": [],
            "fund_keywords": ["æœ‰è‰²é‡‘å±", "é“œ", "é“", "é”‚", "ç¨€åœŸ", "èµ„æº", "çŸ¿ä¸š", "æ–°èƒ½æº"],
            "reason": "éæ´²å›½å®¶é”‚çŸ¿å‡ºå£é™åˆ¶+å…¨çƒç¨€åœŸä¾›åº”ç´§å¼ ï¼Œæ¨å‡æœ‰è‰²é‡‘å±ä»·æ ¼",
            "advice": "æœ‰è‰²é‡‘å±+é”‚ç”µETFå…³æ³¨ä¾›ç»™ç«¯",
        },
    },
    {
        "key_keywords": ['OPEC', 'opec', 'å‡äº§', 'æ²¹ä»·', 'è‹±ä¼¦ç‰¹', 'å¸ƒä¼¦ç‰¹åŸæ²¹', 'oil price', 'crude'],
        "key_sectors": {'åŸæ²¹', 'æ²¹æ°”', 'èƒ½æº'},
        "template": {
            "title": "OPEC+äº§é‡æ”¿ç­–å½±å“æ²¹ä»·",
            "category": "commodity",
            "concepts": ["åŸæ²¹"],
            "sentiment": 0.2,
            "impact": 3,
            "sectors_positive": ["åŸæ²¹", "èƒ½æº", "æ²¹æ°”", "å¤§å®—å•†å“"],
            "sectors_negative": ["èˆªç©º", "äº¤é€š"],
            "fund_keywords": ["åŸæ²¹", "æ²¹æ°”", "çŸ³æ²¹", "èƒ½æº"],
            "reason": "OPEC+å‡äº§æ”¿ç­–å»¶ç»­ï¼Œæ²¹ä»·ä¸­æ¢ä¸Šç§»ï¼Œèƒ½æºè‚¡å—ç›Š",
            "advice": "æ²¹æ°”åŸºé‡‘å…³æ³¨ä¾›ç»™ç«¯å˜åŒ–",
        },
    },
]


def _ensure_geopolitical_events(events, all_news, now):
    """ç¡®ä¿é‡å¤§åœ°ç¼˜äº‹ä»¶å§‹ç»ˆè¢«è¿½è¸ª(å³ä½¿LLMæœªå•ç‹¬æå–)"""
    # æ±‡æ€»æ‰€æœ‰æ–°é—»æ ‡é¢˜æ–‡æœ¬ç”¨äºå…³é”®è¯æ£€æµ‹
    all_text = ' '.join(n.get('title', '') for n in all_news).lower()
    
    # æ±‡æ€»å·²æœ‰äº‹ä»¶è¦†ç›–çš„æ¿å—
    all_sectors = set()
    for e in events:
        all_sectors.update(e.get('sectors_positive', []))
        all_sectors.update(e.get('sectors_negative', []))
    
    added = 0
    for fb in _GEOPOLITICAL_FALLBACKS:
        # æ£€æŸ¥è¯¥åœ°ç¼˜ä¸»é¢˜æ˜¯å¦å·²è¢«åŠ¨æ€äº‹ä»¶å®Œå…¨è¦†ç›–(æ‰€æœ‰å…³é”®æ¿å—éƒ½æœ‰å¯¹åº”äº‹ä»¶æ‰è·³è¿‡)
        uncovered = fb['key_sectors'] - all_sectors
        if not uncovered:
            continue
        
        # æ£€æŸ¥æ–°é—»ä¸­æ˜¯å¦æœ‰ç›¸å…³å…³é”®è¯(å³ä½¿LLMæ²¡æå–ï¼Œæ–°é—»ä¸­æœ‰æåŠå°±è¡¥å……)
        kw_found = any(kw.lower() in all_text for kw in fb['key_keywords'])
        
        if kw_found:
            idx = len(events)
            evt = dict(fb['template'])
            evt['id'] = f"evt_{now.strftime('%Y%m%d')}_geo_{idx+1:03d}"
            evt['confidence'] = 0.65
            evt['source'] = "åœ°ç¼˜äº‹ä»¶è¿½è¸ª"
            evt['time'] = now.isoformat()
            events.append(evt)
            added += 1
            print(f"  ğŸŒ è¡¥å……åœ°ç¼˜äº‹ä»¶: {evt['title']} (æ–°é—»ä¸­æ£€æµ‹åˆ°å…³é”®è¯)")
    
    if added:
        print(f"  å…±è¡¥å…… {added} ä¸ªåœ°ç¼˜æ”¿æ²»äº‹ä»¶")
    return events


_KEY_EVENT_TEMPLATES = [
    {
        'name': 'ä¸­ä¸œå†²çª',
        'keywords': ['ä¸­ä¸œ', 'ä¼Šæœ—', 'ä»¥è‰²åˆ—', 'ä¼Šä»¥', 'éœå°”æœ¨å…¹', 'çº¢æµ·', 'houthi', 'iran', 'israel'],
        'title': 'ä¸­ä¸œå±€åŠ¿å‡çº§æ‰°åŠ¨å¸‚åœº',
        'category': 'geopolitics',
        'concepts': ['åŸæ²¹', 'é»„é‡‘', 'å†›å·¥'],
        'sectors_positive': ['åŸæ²¹', 'æ²¹æ°”', 'èƒ½æº', 'é»„é‡‘', 'è´µé‡‘å±', 'å†›å·¥', 'å›½é˜²'],
        'sectors_negative': ['æ¶ˆè´¹', 'èˆªç©º'],
        'fund_keywords': ['åŸæ²¹', 'æ²¹æ°”', 'èƒ½æº', 'é»„é‡‘', 'å†›å·¥'],
        'sentiment': -0.35,
        'impact': 4,
        'reason': 'åœ°ç¼˜å†²çªæŠ¬å‡é¿é™©ä¸é€šèƒ€é¢„æœŸï¼Œæ²¹æ°”ä¸é»„é‡‘æ³¢åŠ¨æ”¾å¤§',
        'advice': 'æ²¹æ°”+é»„é‡‘é˜²å¾¡é…ç½®ï¼Œé¿å…è¿½æ¶¨æ€è·Œ',
    },
    {
        'name': 'ä¼Šæœ—é«˜å±‚çªå‘',
        'keywords': ['ä¼Šæœ—é¢†å¯¼äºº', 'ä¼Šæœ—æ€»ç»Ÿ', 'ä¼Šæœ—é«˜å±‚', 'ä¼Šæœ— é¢†å¯¼äºº', 'tehran', 'assassinated', 'æ­»äº¡', 'é‡è¢­', 'å æœº'],
        'title': 'ä¼Šæœ—é«˜å±‚çªå‘äº‹ä»¶å¼•å‘é¿é™©äº¤æ˜“',
        'category': 'geopolitics',
        'concepts': ['é»„é‡‘', 'åŸæ²¹'],
        'sectors_positive': ['é»„é‡‘', 'è´µé‡‘å±', 'åŸæ²¹', 'æ²¹æ°”', 'å†›å·¥'],
        'sectors_negative': ['æ¶ˆè´¹', 'èˆªç©º'],
        'fund_keywords': ['é»„é‡‘', 'åŸæ²¹', 'æ²¹æ°”', 'å†›å·¥', 'é¿é™©'],
        'sentiment': -0.45,
        'impact': 4,
        'reason': 'ä¸­ä¸œæ”¿æ²»ä¸ç¡®å®šæ€§ä¸Šå‡ï¼Œé£é™©èµ„äº§é£é™©åå¥½ä¸‹é™',
        'advice': 'æé«˜é˜²å¾¡ä»“ä½ï¼Œé‡ç‚¹è§‚å¯Ÿæ²¹ä»·ä¸é‡‘ä»·å…±æŒ¯',
    },
]


def _inject_key_events_with_analyst_views(events, all_news, analyst_views, now):
    """æ³¨å…¥é‡ç‚¹äº‹ä»¶ï¼Œå¹¶èåˆçƒ­é—¨åˆ†æå¸ˆå®æ—¶è§‚ç‚¹"""
    all_text = ' '.join(n.get('title', '') for n in (all_news or [])).lower()
    existing_titles = {e.get('title', '') for e in events}
    added = 0

    for tpl in _KEY_EVENT_TEMPLATES:
        if not any(kw.lower() in all_text for kw in tpl['keywords']):
            continue

        # å·²æœ‰ç›¸åŒä¸»é¢˜åˆ™ä»…å¢å¼ºè§‚ç‚¹å­—æ®µ
        existing = next((e for e in events if tpl['title'] in (e.get('title') or '') or any(c in (e.get('concepts') or []) for c in tpl['concepts'])), None)
        analyst_note = _analyst_snippet(analyst_views, tpl['keywords'], limit=2)

        if existing:
            if analyst_note and 'åˆ†æå¸ˆè§‚ç‚¹' not in (existing.get('reason') or ''):
                existing['reason'] = f"{existing.get('reason', '')}ï¼›åˆ†æå¸ˆè§‚ç‚¹ï¼š{analyst_note}".strip('ï¼›')
            if analyst_note and not existing.get('analyst_view'):
                existing['analyst_view'] = analyst_note
            continue

        idx = len(events)
        evt = {
            'id': f"evt_{now.strftime('%Y%m%d')}_key_{idx+1:03d}",
            'title': tpl['title'],
            'category': tpl['category'],
            'concepts': tpl['concepts'],
            'sentiment': round(tpl['sentiment'], 2),
            'impact': int(tpl['sentiment'] * tpl['impact'] * 4),
            'confidence': 0.78,
            'sectors_positive': tpl['sectors_positive'],
            'sectors_negative': tpl['sectors_negative'],
            'fund_keywords': tpl['fund_keywords'],
            'reason': tpl['reason'],
            'advice': tpl['advice'],
            'source': 'é‡ç‚¹äº‹ä»¶è¿½è¸ª',
            'time': now.isoformat(),
        }
        if analyst_note:
            evt['reason'] = f"{evt['reason']}ï¼›åˆ†æå¸ˆè§‚ç‚¹ï¼š{analyst_note}"
            evt['advice'] = f"{evt['advice']}ï¼ˆå‚è€ƒçƒ­é—¨åˆ†æå¸ˆå®æ—¶è§‚ç‚¹ï¼‰"
            evt['analyst_view'] = analyst_note

        if evt['title'] not in existing_titles:
            events.append(evt)
            existing_titles.add(evt['title'])
            added += 1

    if added:
        print(f"  ğŸ§© æ³¨å…¥é‡ç‚¹äº‹ä»¶: {added} æ¡ï¼ˆå«åˆ†æå¸ˆè§‚ç‚¹èåˆï¼‰")
    return events


def _attach_analyst_views_to_events(events, analyst_views):
    """ä¸ºäº‹ä»¶è¡¥å……åˆ†æå¸ˆè§‚ç‚¹ï¼ˆå³ä½¿ä¸æ˜¯é‡ç‚¹äº‹ä»¶ï¼‰"""
    if not events or not analyst_views:
        return events

    for evt in events:
        if evt.get('analyst_view'):
            continue
        keywords = []
        title = (evt.get('title') or '').strip()
        if title:
            keywords.append(title)
        for c in (evt.get('concepts') or []):
            if c:
                keywords.append(str(c))
        for k in (evt.get('fund_keywords') or []):
            if k:
                keywords.append(str(k))

        # å…³é”®è¯è¿‡å°‘æ—¶ï¼Œç”¨ç±»åˆ«å…œåº•
        if not keywords and evt.get('category'):
            keywords.append(str(evt.get('category')))

        note = _analyst_snippet(analyst_views, keywords, limit=1)
        if note:
            evt['analyst_view'] = note
            if 'åˆ†æå¸ˆè§‚ç‚¹' not in (evt.get('reason') or ''):
                evt['reason'] = f"{evt.get('reason', '')}ï¼›åˆ†æå¸ˆè§‚ç‚¹ï¼š{note}".strip('ï¼›')

    return events


def build_output(llm_result, prev_data, now, all_news=None, xueqiu_data=None, analyst_views=None):
    """ç»„è£…æœ€ç»ˆJSONè¾“å‡º"""
    events = []
    for i, e in enumerate(llm_result.get('events', [])[:12]):
        events.append(enrich_event(e, i, now))

    # === è¡¥å……å¤§å®—å•†å“å¸¸é©»äº‹ä»¶ ===
    events = _ensure_commodity_events(events, now)

    # === è¡¥å……åœ°ç¼˜æ”¿æ²»äº‹ä»¶(æ–°é—»ä¸­æœ‰å…³é”®è¯but LLMæœªæå–çš„) ===
    if all_news:
        events = _ensure_geopolitical_events(events, all_news, now)

    # === æ³¨å…¥é‡ç‚¹äº‹ä»¶ + çƒ­é—¨åˆ†æå¸ˆå®æ—¶è§‚ç‚¹ ===
    if all_news:
        events = _inject_key_events_with_analyst_views(events, all_news, analyst_views or [], now)

    # === å…¨é‡äº‹ä»¶è¡¥å……åˆ†æå¸ˆè§‚ç‚¹ ===
    events = _attach_analyst_views_to_events(events, analyst_views or [])

    # çƒ­åº¦å›¾: è¡¥å……è¶‹åŠ¿
    heatmap = []
    for h in llm_result.get('heatmap', []):
        tag = h.get('tag', '')
        if tag not in MARKET_TAGS:
            continue
        temp = max(0, min(100, h.get('temperature', 50)))
        heatmap.append({
            "tag": tag,
            "temperature": temp,
            "sentiment": round(h.get('sentiment', 0), 2),
            "trend": compute_trend(temp, prev_data, tag),
        })
    # ç¡®ä¿æ‰€æœ‰æ ‡ç­¾éƒ½æœ‰çƒ­åº¦æ•°æ®
    existing_tags = {h['tag'] for h in heatmap}
    for tag in MARKET_TAGS:
        if tag not in existing_tags:
            heatmap.append({"tag": tag, "temperature": 50, "sentiment": 0, "trend": "stable"})
    heatmap.sort(key=lambda x: x['temperature'], reverse=True)

    # å¸‚åœºæ€»è§ˆåˆ†æ•°
    if events:
        avg_sentiment = sum(e['sentiment'] * abs(e['impact']) for e in events) / max(sum(abs(e['impact']) for e in events), 1)
        outlook_score = int(50 + avg_sentiment * 25)
    else:
        outlook_score = 50
    outlook_score = max(10, min(90, outlook_score))

    return {
        "updated_at": now.isoformat(),
        "heatmap": heatmap,
        "events": events,
        "outlook": {
            "period": f"{now.year}å¹´{now.month}æœˆ",
            "summary": llm_result.get('outlook_summary', 'å¸‚åœºç»“æ„æ€§è¡Œæƒ…å»¶ç»­'),
            "score": outlook_score,
        },
        "xueqiu_hotwords": xueqiu_data if xueqiu_data else None,
        "meta": {
            "news_count": 0,  # filled by main
            "sources": [],
            "model": MODEL,
            "refresh_interval_minutes": 30,
            "analyst_views_count": len(analyst_views or []),
        }
    }


# ==================== ä¸»æµç¨‹ ====================

def main():
    now = datetime.now(timezone(timedelta(hours=8)))
    print(f"{'='*50}")
    print(f"åŸºé‡‘åŠ©æ‰‹ - å®è§‚äº‹ä»¶è¿½è¸ªç®¡é“")
    print(f"æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')} CST")
    print(f"æ¨¡å‹: {MODEL}")
    print(f"{'='*50}")

    # 0. é›ªçƒå®æ—¶çƒ­è¯ (ç‹¬ç«‹äºLLMæµç¨‹, å…ˆè¡ŒæŠ“å–)
    print("\nâ„ï¸ [0/3] é›ªçƒå®æ—¶çƒ­è¯...")
    xueqiu_data = fetch_xueqiu_hotwords()
    if is_valid_xueqiu_data(xueqiu_data):
        xq_count = len(xueqiu_data.get('hotwords', []))
        print(f"  âœ… é›ªçƒçƒ­è¯: {xq_count} æ¡")
    else:
        print("  âš ï¸ é›ªçƒçƒ­è¯: æŠ“å–å¤±è´¥æˆ–ä¸ºç©º, å°†åœ¨è¾“å‡ºé˜¶æ®µå°è¯•å›é€€ç¼“å­˜")

    # 1. å¤šæºæŠ“å–æ–°é—»
    print("\nğŸ“¡ [1/3] æŠ“å–è´¢ç»æ–°é—»...")
    all_news = []
    sources_ok = []

    for name, fetcher in [
        ('æ–°æµªè´¢ç»', fetch_sina_news),
        ('ä¸œæ–¹è´¢å¯Œ', fetch_eastmoney_news),
        ('è´¢è”ç¤¾', fetch_cls_news),
        ('BBC', fetch_rss_bbc),
        ('Reuters', fetch_rss_reuters),
        ('AlJazeera', fetch_rss_aljazeera),
        ('FT', fetch_rss_ft),
        ('è§‚å¯Ÿè€…ç½‘', fetch_guancha_news),
    ]:
        try:
            items = fetcher()
            if items:
                all_news.extend(items)
                sources_ok.append(name)
                print(f"  âœ… {name}: {len(items)} æ¡")
            else:
                print(f"  âš ï¸ {name}: 0 æ¡")
        except Exception as e:
            print(f"  âŒ {name}: {e}")

    if not all_news:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–°é—», ä¿ç•™ä¸Šæ¬¡æ•°æ®")
        sys.exit(0)

    # å»é‡
    seen = set()
    deduped = []
    for n in all_news:
        key = re.sub(r'\W', '', n['title'])[:30]
        if key not in seen:
            seen.add(key)
            deduped.append(n)
    print(f"\n  æ€»è®¡: {len(all_news)} æ¡, å»é‡å: {len(deduped)} æ¡")

    # 1.5 è¯»å–çƒ­é—¨åˆ†æå¸ˆå®æ—¶è§‚ç‚¹ï¼ˆç”¨äºé‡ç‚¹äº‹ä»¶å¢å¼ºï¼‰
    analyst_views = _extract_analyst_views(max_items=16)
    if analyst_views:
        print(f"  ğŸ§  çƒ­é—¨åˆ†æå¸ˆè§‚ç‚¹: {len(analyst_views)} æ¡")
    else:
        print("  âš ï¸ çƒ­é—¨åˆ†æå¸ˆè§‚ç‚¹: 0 æ¡ï¼ˆå°†ä»…ä½¿ç”¨æ–°é—»è¯­ä¹‰ï¼‰")

    # 2. LLM ç»“æ„åŒ–
    print("\nğŸ§  [2/3] AIç»“æ„åŒ–æå–...")
    llm_result = call_llm(deduped)

    if not llm_result:
        print("\nâŒ LLMåˆ†æå¤±è´¥, ä¿ç•™ä¸Šæ¬¡æ•°æ®")
        sys.exit(0)

    print(f"  æå–äº‹ä»¶: {len(llm_result.get('events', []))} æ¡")
    print(f"  çƒ­åº¦æ ‡ç­¾: {len(llm_result.get('heatmap', []))} ä¸ª")

    # 3. ç»„è£…è¾“å‡º
    print("\nğŸ“¦ [3/3] ç»„è£…è¾“å‡º...")
    prev_data = load_previous()
    if not is_valid_xueqiu_data(xueqiu_data):
        prev_xq = prev_data.get('xueqiu_hotwords') if isinstance(prev_data, dict) else None
        if is_valid_xueqiu_data(prev_xq):
            xueqiu_data = prev_xq
            print(f"  â™»ï¸ ä½¿ç”¨ä¸Šæ¬¡é›ªçƒç¼“å­˜: {len(xueqiu_data.get('hotwords', []))} çƒ­è¯")

    output = build_output(
        llm_result,
        prev_data,
        now,
        all_news=deduped,
        xueqiu_data=xueqiu_data,
        analyst_views=analyst_views,
    )
    output['meta']['news_count'] = len(deduped)
    output['meta']['sources'] = list(sources_ok)

    # ä¿æŒæ¥æºæ ‡è®°ä¸é›ªçƒæ•°æ®ä¸€è‡´ï¼šä»…åœ¨æœ‰æœ‰æ•ˆé›ªçƒçƒ­è¯/çƒ­è‚¡æ—¶ä¿ç•™â€œé›ªçƒâ€
    has_valid_xq = is_valid_xueqiu_data(output.get('xueqiu_hotwords'))
    if has_valid_xq:
        if 'é›ªçƒ' not in output['meta']['sources']:
            output['meta']['sources'].append('é›ªçƒ')
    else:
        output['meta']['sources'] = [s for s in output['meta']['sources'] if s != 'é›ªçƒ']

    # å†™å…¥æ–‡ä»¶
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*50}")
    print(f"âœ… è¾“å‡º: {OUTPUT_PATH}")
    print(f"   äº‹ä»¶: {len(output['events'])} æ¡")
    print(f"   çƒ­åº¦: {len(output['heatmap'])} æ ‡ç­¾")
    print(f"   æ€»è§ˆ: {output['outlook']['summary']}")
    print(f"   åˆ†æ•°: {output['outlook']['score']}")
    if is_valid_xueqiu_data(xueqiu_data):
        print(f"   é›ªçƒ: {len(xueqiu_data.get('hotwords', []))} çƒ­è¯ / {len(xueqiu_data.get('hot_stocks', []))} çƒ­è‚¡")
    print(f"{'='*50}")


if __name__ == '__main__':
    main()
